[["index.html", "Kitab at-Tabakh Intro", " Kitab at-Tabakh Peter Geelan-Small 28/09/2020 Intro This is a collection of miscellaneous statistics-related recipes and notes. "],["mutanawwiyat.html", "1 Mutanawwiyat 1.1 Confounders 1.2 Exponential models 1.3 Power function 1.4 ROC curves 1.5 Variable selection", " 1 Mutanawwiyat blockquote { margin-left: 36px; font-size:14px; } 1.1 Confounders https://en.wikipedia.org/wiki/Confounding https://www.ucl.ac.uk/child-health/short-courses-events/about-statistical-courses/research-methods-and-statistics/chapter-1-content-0 1.2 Exponential models \\(y = A e^{bt}\\) \\(t = 0, y = 5\\) \\(A = 5\\) Three states: \\(t = 10, y_1 = 500, y_2 = 100, y_3 = 25\\) \\(b = (\\log y - \\log 5) / t\\) \\(b_1 = (\\log 500 - \\log 5) / 10 = 0.461\\) \\(b_2 = (\\log 100 - \\log 5) / 10 = 0.300\\) \\(b_3 = (\\log 25 - \\log 5) / 10 = 0.161\\) tt = 0:15 y1 &lt;- A * exp(b_1 * tt) y2 &lt;- A * exp(b_2 * tt) y3 &lt;- A * exp(b_3 * tt) dd1 &lt;- data.frame(time = rep(tt, 3), y = c(y1, y2, y3), batch = rep(1:3, each = length(tt))) dd1$batch &lt;- factor(dd1$batch) ggplot(dd1, aes(x = time, y = y, colour = batch)) + geom_line() \\(y = A e^{bt}\\) is the same as \\(\\log y = \\log A + bt\\) dd1$log_y &lt;- log(dd1$y) ggplot(dd1, aes(x = time, y = log_y, colour = batch)) + geom_line() ggplot(dd1, aes(x = time, y = y, colour = batch)) + geom_line() + coord_trans(y = &#39;log&#39;) + scale_x_continuous(breaks = c(0:8*2)) + scale_y_continuous(breaks = c(10, 20, 40, 80, 160, 325, 650, 1300, 2600, 5200)) For a one unit increase in time, the change in \\(y\\) is: \\(\\log y_j - \\log y_i = b\\) So, \\(y_j = e^b y_i\\) For batch 1, \\(e^b = e^{0.461} = 1.59\\), so for each one unit increase in time, \\(y\\) increases by a factor of 1.59 or by 59%. Values of batch 1 at times from 0 to 15: round(y1, digits = 3) ## [1] 5.000 7.924 12.559 19.905 31.548 50.000 79.245 ## [8] 125.594 199.054 315.479 500.000 792.447 1255.943 1990.536 ## [15] 3154.787 5000.000 Ratios of \\(y_{1, k} \\, / \\, y_{1, k-1}\\) for some values are shown below, noting that \\(e^{b_1} =\\) 1.585. For an increase of two units in time, \\(\\log y_j - \\log y_i = 2b\\), so \\(y_j = e^{2b} y_i\\). \\(\\;\\;e^{2b_1} =\\) 2.512 rat_21 &lt;- round(y1[2] / y1[1], digits = 3) rat_32 &lt;- round(y1[3] / y1[2], digits = 3) rat_10_9 &lt;- round(y1[10] / y1[9], digits = 3) rat_15_13 &lt;- round(y1[15] / y1[13], digits = 3) Times Ratio 1 &amp; 2 1.585 2 &amp; 3 1.585 9 &amp; 10 1.585 13 &amp; 15 2.512 Doubling time When has \\(y\\) reached double its initial value - i.e.Â when is \\(y = 10\\)? Doubling time is \\(t_{\\mathrm{dbl}}\\). \\(e^{bt_{\\mathrm{dbl}}} = 2\\) \\(t_{\\mathrm{dbl}} = \\log 2 / b\\) For batch 1: \\(t_{\\mathrm{dbl}} = \\log 2 / 0.461 = 1.51\\) For batch 2: \\(t_{\\mathrm{dbl}} = \\log 2 / 0.300 = 2.31\\) For batch 1: \\(t_{\\mathrm{dbl}} = \\log 2 / 0.161 = 4.31\\) t10 &lt;- 5 t12 &lt;- t10 + t_dbl_1 t22 &lt;- t10 + t_dbl_2 t32 &lt;- t10 + t_dbl_3 y11 &lt;- round(A * exp(b_1 * t10), digits = 2) y21 &lt;- round(A * exp(b_2 * t10), digits = 2) y31 &lt;- round(A * exp(b_3 * t10), digits = 2) y12 &lt;- round(A * exp(b_1 * t12), digits = 2) y22 &lt;- round(A * exp(b_2 * t22), digits = 2) y32 &lt;- round(A * exp(b_3 * t32), digits = 2) Table below shows value of \\(y\\) at time = 5 and then at time = (5 + doubling time). Batch time 1 ( = 5) time 2 ( = 5 + t_dbl) 1 50 100 2 22.36 44.72 3 11.18 22.36 1.3 Power function \\(y = Cx^b\\) This becomes \\(\\log y = \\log C + \\log(x^b)\\) or \\(\\log y = c + b \\log x\\). Take \\(C = 2\\) and \\(b_4 = 0.8, b_5 = 0.4, b_6 = 0.2\\) C &lt;- 2 b_4 &lt;- 0.8 b_5 &lt;- 0.4 b_6 &lt;- 0.2 tt = seq(0.0001, 15, by = 0.2) y4 &lt;- C * tt^b_4 y5 &lt;- C * tt^b_5 y6 &lt;- C * tt^b_6 dd2 &lt;- data.frame(time = rep(tt, 3), y = c(y4, y5, y6), batch = rep(1:3, each = length(tt))) dd2$batch &lt;- factor(dd2$batch) ggplot(dd2, aes(x = time, y = y, colour = batch)) + geom_line() dd2$log_y &lt;- log(dd2$y) dd2$log_time &lt;- log(dd2$time) ggplot(dd2, aes(x = log_time, y = log_y, colour = batch)) + geom_line() ggplot(dd2, aes(x = time, y = y, colour = batch)) + geom_line() + coord_trans(x = &#39;log&#39;) + coord_trans(y = &#39;log&#39;) + scale_x_continuous(breaks = c(0:8*2)) + scale_y_continuous(breaks = c(0:6*3)) 1.4 ROC curves Example from Bland. library(ggplot2) library(pROC) ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var roc_ami &lt;- read.csv(&quot;data/roc bland.csv&quot;, header = T) str(roc_ami) ## &#39;data.frame&#39;: 120 obs. of 2 variables: ## $ CK : int 23 33 36 37 37 41 41 41 42 42 ... ## $ Disease: int 0 0 0 0 0 0 0 0 0 0 ... summary(roc_ami) ## CK Disease ## Min. : 23.0 Min. :0.000 ## 1st Qu.: 61.5 1st Qu.:0.000 ## Median : 101.5 Median :0.000 ## Mean : 410.0 Mean :0.225 ## 3rd Qu.: 257.0 3rd Qu.:0.000 ## Max. :11138.0 Max. :1.000 ## Labels: Angina is &quot;unstable angina&quot;; ## AMI is &quot;acute myocardial infarction&quot; roc_ami$DiseaseF &lt;- factor(roc_ami$Disease, labels = c(&quot;Angina&quot;, &quot;AMI&quot;)) head(roc_ami) ## CK Disease DiseaseF ## 1 23 0 Angina ## 2 33 0 Angina ## 3 36 0 Angina ## 4 37 0 Angina ## 5 37 0 Angina ## 6 41 0 Angina boxplot(log10(roc_ami$CK) ~ roc_ami$DiseaseF) ggplot(roc_ami, aes(x = DiseaseF, y = log10(CK))) + ylim(1, 4.2) + geom_jitter(width = 0.1, height = 0.1) ami_roc_curve &lt;- roc(Disease ~ CK, data = roc_ami) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases ami_roc_curve ## ## Call: ## roc.formula(formula = Disease ~ CK, data = roc_ami) ## ## Data: CK in 93 controls (Disease 0) &lt; 27 cases (Disease 1). ## Area under the curve: 0.9753 auc(Disease ~ CK, data = roc_ami) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases ## Area under the curve: 0.9753 ci(ami_roc_curve) ## 95% CI: 0.9424-1 (DeLong) ami_roc_plot &lt;- roc(Disease ~ CK, data = roc_ami, plot = T) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases ami_roc_plot ## ## Call: ## roc.formula(formula = Disease ~ CK, data = roc_ami, plot = T) ## ## Data: CK in 93 controls (Disease 0) &lt; 27 cases (Disease 1). ## Area under the curve: 0.9753 plot.roc(Disease ~ CK, data = roc_ami, ci = T, of = &quot;thresholds&quot;) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases Compute the confidence interval (CI) of the sensitivity at the given specificity points ci1 &lt;- ci.se(ami_roc_curve, boot.n = 10) plot(ami_roc_curve) plot(ci1, col = &quot;blue&quot;) coords(ami_roc_plot, x = &quot;all&quot;) ## threshold specificity sensitivity ## 1 -Inf 0.00000000 1.00000000 ## 2 28.0 0.01075269 1.00000000 ## 3 34.5 0.02150538 1.00000000 ## 4 36.5 0.03225806 1.00000000 ## 5 39.0 0.05376344 1.00000000 ## 6 41.5 0.08602151 1.00000000 ## 7 42.5 0.10752688 1.00000000 ## 8 44.0 0.11827957 1.00000000 ## 9 46.0 0.12903226 1.00000000 ## 10 47.5 0.13978495 1.00000000 ## 11 48.5 0.17204301 1.00000000 ## 12 50.5 0.18279570 1.00000000 ## 13 52.5 0.21505376 1.00000000 ## 14 53.5 0.22580645 1.00000000 ## 15 55.5 0.23655914 1.00000000 ## 16 57.5 0.25806452 1.00000000 ## 17 59.0 0.29032258 1.00000000 ## 18 61.0 0.32258065 1.00000000 ## 19 62.5 0.33333333 1.00000000 ## 20 64.0 0.35483871 1.00000000 ## 21 65.5 0.37634409 1.00000000 ## 22 66.5 0.38709677 1.00000000 ## 23 69.0 0.39784946 1.00000000 ## 24 71.5 0.40860215 1.00000000 ## 25 72.5 0.43010753 1.00000000 ## 26 74.0 0.45161290 1.00000000 ## 27 77.5 0.46236559 1.00000000 ## 28 81.5 0.48387097 1.00000000 ## 29 83.5 0.49462366 1.00000000 ## 30 84.5 0.50537634 1.00000000 ## 31 85.5 0.51612903 1.00000000 ## 32 87.0 0.52688172 1.00000000 ## 33 88.5 0.55913978 1.00000000 ## 34 89.5 0.56989247 1.00000000 ## 35 90.5 0.56989247 0.96296296 ## 36 92.5 0.58064516 0.96296296 ## 37 94.5 0.60215054 0.96296296 ## 38 96.0 0.61290323 0.96296296 ## 39 98.5 0.62365591 0.96296296 ## 40 101.5 0.63440860 0.96296296 ## 41 103.5 0.64516129 0.96296296 ## 42 104.5 0.65591398 0.96296296 ## 43 106.0 0.67741935 0.96296296 ## 44 107.5 0.68817204 0.96296296 ## 45 108.5 0.69892473 0.96296296 ## 46 110.0 0.70967742 0.96296296 ## 47 112.5 0.72043011 0.96296296 ## 48 115.0 0.73118280 0.96296296 ## 49 117.0 0.74193548 0.96296296 ## 50 119.5 0.75268817 0.96296296 ## 51 121.5 0.77419355 0.96296296 ## 52 124.0 0.78494624 0.96296296 ## 53 128.0 0.79569892 0.96296296 ## 54 134.5 0.81720430 0.96296296 ## 55 144.5 0.82795699 0.96296296 ## 56 152.5 0.83870968 0.96296296 ## 57 156.0 0.84946237 0.96296296 ## 58 159.5 0.86021505 0.96296296 ## 59 169.0 0.87096774 0.96296296 ## 60 178.0 0.88172043 0.96296296 ## 61 184.0 0.89247312 0.96296296 ## 62 192.0 0.90322581 0.96296296 ## 63 197.0 0.90322581 0.92592593 ## 64 212.0 0.91397849 0.92592593 ## 65 229.0 0.92473118 0.92592593 ## 66 244.5 0.93548387 0.92592593 ## 67 277.0 0.95698925 0.92592593 ## 68 299.5 0.96774194 0.92592593 ## 69 304.5 0.96774194 0.88888889 ## 70 309.0 0.97849462 0.88888889 ## 71 318.0 0.97849462 0.85185185 ## 72 330.0 0.97849462 0.81481481 ## 73 341.0 0.97849462 0.77777778 ## 74 348.0 0.97849462 0.74074074 ## 75 350.0 0.97849462 0.70370370 ## 76 355.5 0.98924731 0.70370370 ## 77 361.5 1.00000000 0.70370370 ## 78 370.0 1.00000000 0.66666667 ## 79 383.5 1.00000000 0.62962963 ## 80 394.0 1.00000000 0.59259259 ## 81 471.5 1.00000000 0.55555556 ## 82 561.0 1.00000000 0.51851852 ## 83 603.0 1.00000000 0.48148148 ## 84 638.5 1.00000000 0.44444444 ## 85 771.0 1.00000000 0.40740741 ## 86 928.0 1.00000000 0.37037037 ## 87 988.5 1.00000000 0.33333333 ## 88 1079.0 1.00000000 0.29629630 ## 89 1300.5 1.00000000 0.25925926 ## 90 1706.5 1.00000000 0.22222222 ## 91 2047.0 1.00000000 0.18518519 ## 92 2169.5 1.00000000 0.14814815 ## 93 2622.0 1.00000000 0.11111111 ## 94 5317.0 1.00000000 0.07407407 ## 95 9364.0 1.00000000 0.03703704 ## 96 Inf 1.00000000 0.00000000 1.5 Variable selection 1.5.1 Davison 2003, Statistical Models pp.Â 400-401 Automatic variable selection methods can fit complicated models to random data. They are widely used but have no theoretical basis. The rules used for deciding which variables to include vary and are arbitrary - various cut-offs for t or F values are used, for example; sometimes AIC is used. These methods can be useful for screening. Knowledge of the system being studied is essential in model building. p.Â 402 Some measure of how well a model fits is a reasonable criterion to use. Residual SS decreases the more terms are added, so this is not satisfctory. A useful approach is to penalise a model according to its complexity and balance this against a measure of how well the model fits the data. AIC is one approach of this type. It provides a balance between the fit of the model and its simplicity. AICc, the corrected AIC, is best when the number of parameters is comparable to the number of rows of data (i.e.Â p is comparable to n). p.Â 404 There may be a number of models with similar AICcs. If a single model is needed, use expert knowledge to choose it, if possible. If more than one model is plausible, then accept that as the conclusion and discuss them. p.Â 405 If automatic variable selection methods are used, making inferences after this model selection is restricted. The only covariates for which standard confidence intervals are reliable are the ones where the evidence that they be included in the model is very strong. 1.5.2 Faraway 2014, Linear Models with R p.Â 153 Do not use testing-based variable selection methods. With methods that are based on P values, so much multiple testing results in P values losing their usual meaning so that the overall process lacks validity. The testing-based procedures are not directly related to the overall objectives, namely, to find the best prediction or explanatory model. Variable selection inflates the statistical significance of variables that are kept in the model. Variables that are dropped are not necessarily unrelated to the response variable; they just do not contribute any extra explanatory value over the ones already in the model. The point of this is a bit unclear. What do other writers say about this? p.Â 159 Use information-criterion-based methods for model selection (e.g.Â AIC, BIC, adjusted \\(R^2\\), Mallowâs \\(C_p\\)) If a number of models fit about the same as one another, accept it! Look at whether they each provide different but plausible explanations of the response they give similar predictions one or some have better model diagnostic features one or some include predictors that are cheaper or easier to measure p.Â 161 Problems in having too many predictors in a regression model include: collinearity - this can impede how well a model explains the response the quality of predictions from the model can be reduced (Faraway probably means by this that the variance of predictions is increased) The apparent contradiction is that more predictors should mean more information! There are methods that help to shrink this extra information into a useful form: PCA partial least squares (p.Â 172) this constructs orthogonal linear combinations of predictors as new predictors these combinations are explicitly constructed to predict Y as well as possible ridge regression (p.Â 174) it produces coefficients which are not very large, which is reasonable if you have a lot of candidate predictors and you think many of them have an effect on the response this shrinks coefficients, which are normalised (i.e.Â centred by their means and scaled by their SDs), towards zero itâs a form of penalised regression LASSO 1.5.3 Faraway 2016, Extending the Linear Model with R, 2nd ed. p.Â 203 For linear mixed models, information-based criteria can be used. However, there are some problems with LMMs: because of dependence in the data, the effective sample size is less than the total number of cases it isnât clear how to count the number of fixed effect parameters and random effect parameters together most information-criteria measures are based on the likelihood, which does not behave well when parameters are estimated at the boundary of the parameter space as can happen with variance components The AIC can be used when the models to be compared differ only in their fixed effects. If random effects also vary, how to count the number of parameters is a problem. The BIC, which replaces the penalty of \\(2 \\; \\times\\) no. of parameters in the AIC with the term, \\(p\\log(n)\\), can also be used. It penalises larger models more heavily than the AIC and favours smaller models than the AIC. 1.5.4 Weisberg 2014, Applied Linear Regression, 4th ed. pp.Â 234-235 Methods for selecting variables in regression modelling depend on the purpose of the analysis: what is the effect of a focal predictor? - examine the effect of a focal predictor, or a few predictors, on a response - including extra predictors apart from those of interest could help the interpretation or increase the precision of tests and estimates; including too many could decrease the precision which of a number of potential predictors are active? - determine the predictors that are associated with the response - i.e.Â distinguish the active predictors from the inactive ones; (estimating coefficients and testing whether coefficients are different from zero are not the main point of âvariable discoveryâ, p.Â 242) predict values of a response using the predictors - including too many predictors could produce inaccurate predictions because too much detail in the observed data is incorporated (i.e.Â overfitting); not including enough predictors can produce inaccurate predictions if important predictors are excluded (i.e.Â biased predictions, as mentioned by someone below) Looking for active predictors pp.Â 237ff One approach is looking at groups of predictors and picking one that maximises some selection criterion. Common criterion is AIC (as well as BIC). p.Â 239 Stepwise methods these methods are not guaranted to give you the model with the optimal value of the selection criterion but they can be useful (p.Â 240) estimating coefficients and testing whether coefficients are different from zero are not the main point of âvariable discoveryâ (p.Â 242) after selecting a subset of variables, the \\(t\\) values and associated \\(P\\) values for coefficients cannot be trusted (as the \\(t\\) values may not follow a \\(t\\) distribution) methods that select subsets exaggerate significance of, for example, \\(F\\) statistics comparing models and \\(t\\) statistics for coefficients in models p.Â 244 Regularised methods - LASSO (incl.Â variant, elastic net) work on the basis that as few predictors as possible are needed to model a response work well on randomly sampled data and model with no factors or interaction p.Â 245 Developing a model for prediction Possible approaches model averaging (incl.Â Bayesian model averaging) - v. Hoeting tutorial cross-validation general question of getting predictions from training data has led to development of machine learning; methods include neural networks, random forests, â¦ 1.5.5 Harrell 2015, Regression Modeling Strategies, 2nd ed. pp.Â 67-68 Stepwise methods of variable selection have no statistical justification. They result in: \\(R^2\\) values that are too high \\(F\\) and \\(\\chi^2\\) statistics that are not distributed according to those distributions P values that are too small SEs that are too small model coefficients that are too large p.Â 69-70 Do not use stepwise variable selection methods; use full-model fits or data reduction methods instead. If you have to use stepwise variable selection: first of all, fit the full model and do a global test of no regression if this global test is not significant, it is not justifiable to pick out individually significant predictors if the global test is significant, use a stopping rule based on the AIC - i.e.Â smallest AIC (but there are no stopping rules for data-driven variable selection); if someone insists on using a stopping rule based on P values, use \\(\\alpha = 0.5\\) use backwards elimination rather than forwards selection it ensures you see the full model, which is the only one with accurate SEs, residual MS and P values Lawless and Singhalâs method (implemented with fastbw in rms package) is very efficent p.Â 70 Bootstrapping can help you choose between the full and a reduced model. pp.Â 71-72 Sometimes variables can be grouped (by subject matter or empirical correlations), tested as a group and kept or removed. Possibly the most accurately measured variable in a group can be retained (v. p.Â 79f). Lasso is a useful penalised estimation technique - it forces some coefficient estimates to be zero and in this way carries out variable selection, while also, however, shrinking the remaining coefficients to take account of the overfitting caused by using a data-based model selection method Screening each variable singly and using only individually significant variables in a multivariable model can miss variables that become important only after adjusting for other variables. Sample size, overfitting â¦ âOverfittingâ happens when the model fits some of the noise and not just the signal; it also happens when you find questionable associations between explanatory variables and the response variable. A good general recommendation where there is a continuous response variable, for example, is to have no more than one candidate predictor variable per 15 observations (this includes interaction terms, if any). ??? df or terms? If there are explanatory variables with a narrow range of values, youâll need a larger sample than the above recommendation. p.Â 78 (In section on âshrinkageâ) Penalised methods are very good ways to deal with the âtoo many variables, too little dataâ problem (more detail in s. 9.10). Some useful shrinkage methods are: ridge regression penalised ML estimation Cross-validation or AIC has to be used to choose the penalty factor. p.Â 78 Collinearity When one predictor can be predicted well from the others or expressed as a combination of the others, collinearity is present (i.e.Â if \\(\\textbf{Xa} = \\textbf{0}\\), where \\(\\textbf{X}\\) is the incidence matrix and \\(\\textbf{a}\\) is a vector of constants). This can inflate the SEs of the regression coefficients. It can also make selecting âimportantâ variables unreliable. When groups of highly correlated variables are identified, they can be tested as a whole set with a multiple d.f. test, rather than using a one d.f. test on a single predictor. To quantify collinearity, use the variance inflation factor. p.Â 79 Data reduction Use expert knowledge and previous research to remove unimportant variables from the candidate variables Remove variables that have distributions that are too narrow Remove variables that have a large number of missing values pp.Â 89-90 Recommended approaches Fit a full model without removing non-significant predictors Use data reduction methods to reduce dimensionality Use shrinkage (i.e.Â penalised estimation) to fit a large model and not worry about sample size pp.Â 94ff Harrellâs summary Developing models to find active predictors Collect as much data as possible; ensure predictor variables have wide distributions Frame good hypotheses that specify predictor variables that are relevant, including interactions, if appropriate - do not use the response variable in this process of specifying predictors either graphically, using descriptive statistics or doing hypothesis tests of estimates (i.e.Â use expert knowledge to âscreenâ potential candidate predictors) Impute values as appropriate - see text for details Either use cross-validation or bootstrapping, or hold out some sample data as test data The following steps need to be carried out on each bootstrap or cross-validation sample. When you can test the model for complexity in a structured way, you might be able to simplify it without using penalisation methods - e.g.Â test a group of predictors based on one P value (which is hopefully outside range of 0.05 to 0.2) Check additivity by testing all prespecified interaction terms; if the test is unequivocal (e.g.Â P &gt; 0.3), drop all interaction terms. Developing models for estimating effects These are models for getting point or interval estimates for the response at various combinations of the predictors Parsimonious models are less important here, as including all predictors means there is a greater chance the CI for the effect of interest actually is the size stated; on the other hand, including unimportant predictors increases the variance of predicted values Developing models for testing hypotheses Very similar strategy as for models for estimating effects Parsimony has little importance as a full model fit, with non-significant variables, gives more accurate P values for variables of interest It is important to consider interactions; a well-defined test is whether an interaction should be included or not Model validation is not necessary p.Â 118 Simplifying the final model by approximation A model with all prespecified terms usually produces the most accurate predictions on new data Predictions are conditional on all predictors If a predictor is not significant, predictions can be obtained by using weights, based on the fraction of values of each level of that predictor - if a factor - in the data, to make the predictions unconditional on that predictor (e.g.Â average over categories of ethnicity). If there are several non-significant predictors that need to be averaged over in the above way, the process is unwieldy. What to do about this is a bit over the top! 1.5.6 David Wartonâs draft book, chapter 5 p.Â 121 Variable selection or model selection is a type of inference. Inference can take various forms, including: hypothesis testing - seeing whether data are consistent with a particular hypothesis parameter estimation - finding a plausible range of values (confidence interval) for a parameter of interest variable (model) selection - finding the set of predictor variables that characterise the true process by which the data are generated (inference by using a sample to draw general conclusions about how well a set of explanatory variables can predict) pp.Â 118-119 Purpose of model selection: maximise predictive capability - i.e.Â find the model that would give best predictions of the response given new data You need to choose the ârightâ number of predictors - i.e.Â the ârightâ level of model complexity. A model that is too small gives biased predictions. A model that is too big results in predictions with increased variance. This is because the model fits noise as well as signal. Bias-variance trade-off is inevitable. How find the right balance? It depends on: how much data youâve got how relatively complex the models being compared are how strong the signal is in each model R-squared values cannot take account of how complex models are. Using P values to remove non-significant predictors is not appropriate as there is no real a priori hypothesis being tested. p.Â 121 (again) Comparing a lot of models is of doubtful value as using the data alone to produce the best model is unreliable. Itâs important to simplify potential models - use expert knowledge to remove unnecessary predictors. Comparing predictive models The easiest way to compare models is to see how well they predict from new data - this is âvalidationâ. Get new data or split data into a training set and a test set - the split needs to be random, the two sets must be independent. (\\(K\\)-fold) cross-validation can be used here (p.Â 125) - split the data into \\(K\\) groups and use each group once as the test data, fitting \\(K\\) models in all; in each modelling run, hold out the particular test group, fit the model to the remaining \\(K-1\\) groups combined and then compare the model predictions to the test observations; pool measures of predictive performance across all the runs. How do you choose the size of the training sample? Maybe along the lines of Shaoâs method - as sample size increases, use a larger number of observations but a smaller proportion of the total sample size. Some suggestions: leave-one-out (i.e.Â \\(N\\)-fold c.v.) if n &lt; 20 10-fold c.v. for \\(20 &lt; n &lt; 100\\) 5-fold c.v. for \\(n &gt; 100\\) How do you measure predictive performance? For linear regression (assuming constant variance) use mean squared error Use the whole data set (no splits) and penalise larger models by means of an information criterion - e.g.Â AIC, BIC (p.Â 127) what about the GIC ??? AIC overfits and is good if you want a prediction model; BIC does not overfit and is good for variable selection Information criteria - advantages and disadvantages no random split in data so same answer and simpler to interpret less intuitive than cross-validation as they judge predictive capability on new data indirectly (as there is no new data!) they rely on the model being close to the correct model; cross-validation requires only that the training and test data are independent p.Â 130 Subset selection No clear winner among forwards, backwards or all subsets methods All subsets more comprehensive but because so many models may be involved, thereâs no guarantee it will yield best model if there are lots of explanatory variables, itâs not practical Backwards elimination not a good idea if there are a large number of explanatory variables, as full model could be quite unstable with some parameters that are not well estimated Forwards selection with lots of explanatory variables, this might be a better place to start not a good method if the optimal model ends up with a lot of terms, as there are lots of steps in getting to that model where things can go wrong Multi-collinearity This can undermine how stepwise methods work as the chance of a particular term being added to the model drops substantially if itâs correlated with a term already in the model p.Â 134 Penalised estimation A more modern and good way to do subset selection - e.g.Â LASSO Parameter estimates are pushed towards zero - this introduces bias but reduces variance, so thereâs a bias-variance trade-off here Penalised estimation is good when: you want to predict the response, as this method reduces variance of predictions you have lots of terms in your model or not a large sample relative to the number of terms LASSO does variable selection by forcing some parameter estimates to be zero - if the parameter estimate for a term is zero, the terms drops out of the model LASSO - advantages and disadvantages it does model selection as part of its routine and simplifies this model selection to estimating a single nuisance parameter that is part of the penalty term it produces good predictions by reducing variance parameter estimates are, unfortunately, biased standard errors are not easily available 1.5.7 James et al.Â Introduction to Statistical Learning, 2017 (corrected printing) p.Â 30 Assessing model accuracy For linear regression models, a common way of measuring how well the predictions a model makes match the observed data is to use the residual mean square or mean squared error. This says how well the model works on the training data used to fit the model (i.e.Â the training data). Whatâs more interesting and important is how well the model would perform on new data (i.e.Â test data). For example, suppose we have clinical measurements on a group of patients (e.g.Â age, BP, gender, height, weight) and whether each patient has diabetes. We can develop a model using this data to predict risk of diabetes. We are particularly interested whether this model accurately predicts the risk of diabetes for future patients. How can you minimise the test MSE? If test data is available, use this test data set and pick the method that gives smallest MSE. If no test data is available, you could use the method that gives the smallest MSE for the training data â¦. But no! Lots of modelling methods specifically minimise the MSE in the fitting process; these methods can give quite a small MSE for the training set but give a much larger MSE on test data. p.Â 32 - direct quote â¦ as the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function \\(f\\). p.Â 34 Note explanation of bias and variance in context of modelling (or statistical learning). Variance is the amount by which estimated response values would change using different training data. See example in test. Bias is the error caused by approximating a possibly quite complex real-life system by a much simpler model. As the flexibility of a model increases, the variance increases and the bias decreases. p.Â 75 Important questions when doing multiple linear regression: is at least one predictor useful in predicting response? do all predictors, or only some of them, help to explain the response? how well does the model fit the data? given specific values for predictors, what is the predicted response value and how accurate is it? pp.Â 79 and 208 If \\(p &gt; n\\), backward elimination cannot be used, as you cannot fit the full model; forward selection can always be used but if \\(p &gt; n\\), you cannot get a unique solution and can only fit submodels. p.Â 92 Brief overview of potential problems with regression model: non-linearity of relationships between response and predictor correlation of errors (non-independence) non-constant variance of errors outliers points with high leverage collinearity - note very interesting plots showing effect of collinearity on estimated coefficients - fig.Â 3.15 Chapter 6 Linear model selection pp.Â 205-210 All subsets, forward selection, backward elimination and hybrid approaches are outlined. "],["ggplot.html", "2 ggplot 2.1 Vignettes 2.2 Confidence bands 2.3 Confidence bands: fitted models 2.4 Histogram 2.5 Density plot 2.6 Plot structure 2.7 Label legend 2.8 Plot title 2.9 Three-category colours 2.10 Set breaks for scale 2.11 Display table to 2 d.p. 2.12 Redefine factor levels for an individual plot 2.13 Plotting two data sets of different length on one plot", " 2 ggplot 2.1 Vignettes Extending ggplot2 Aesthetic specifications Cheat sheet 2.2 Confidence bands https://janhove.github.io/reporting/2017/05/12/visualising-models-2 2.3 Confidence bands: fitted models Part of this comes from Gordanaâs code for calculating sample size by simulation library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following object is masked from &#39;package:nlme&#39;: ## ## collapse ## The following object is masked from &#39;package:MASS&#39;: ## ## select ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(emmeans) library(ggplot2) library(ggpubr) data(iris, package = &quot;datasets&quot;) str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... table(iris$Species) ## ## setosa versicolor virginica ## 50 50 50 iris_versi &lt;- filter(iris, Species == &quot;versicolor&quot;) summary(iris_versi) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.900 Min. :2.000 Min. :3.00 Min. :1.000 ## 1st Qu.:5.600 1st Qu.:2.525 1st Qu.:4.00 1st Qu.:1.200 ## Median :5.900 Median :2.800 Median :4.35 Median :1.300 ## Mean :5.936 Mean :2.770 Mean :4.26 Mean :1.326 ## 3rd Qu.:6.300 3rd Qu.:3.000 3rd Qu.:4.60 3rd Qu.:1.500 ## Max. :7.000 Max. :3.400 Max. :5.10 Max. :1.800 ## Species ## setosa : 0 ## versicolor:50 ## virginica : 0 ## ## ## iris_versi$Species &lt;- factor(iris_versi$Species) nsamp &lt;- 12 iris_versi_samp &lt;- sample(1:50, size = nsamp, replace = F) pilot &lt;- iris_versi[iris_versi_samp, ] ## Gordana&#39;s simulation code pilot_mod &lt;- lm(Sepal.Length ~ Petal.Length, data = pilot) summary(pilot_mod) ## ## Call: ## lm(formula = Sepal.Length ~ Petal.Length, data = pilot) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.748 -0.264 0.102 0.280 0.440 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9180 1.4077 1.362 0.2029 ## Petal.Length 0.9400 0.3141 2.993 0.0135 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4055 on 10 degrees of freedom ## Multiple R-squared: 0.4725, Adjusted R-squared: 0.4198 ## F-statistic: 8.958 on 1 and 10 DF, p-value: 0.01351 confint(pilot_mod) ## 2.5 % 97.5 % ## (Intercept) -1.2185972 5.054597 ## Petal.Length 0.2402083 1.639792 fit_gg &lt;- ggplot(pilot, aes(x = Petal.Length, y = Sepal.Length)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + ggtitle(&quot;Fitted value with 95 % confidence interval (conditional mean)&quot;) + theme(plot.title = element_text(size = 8, face = &quot;bold&quot;)) print(fit_gg) ## `geom_smooth()` using formula &#39;y ~ x&#39; Check type of band (confidence or prediction) on above ggplot # range of petal length my_range &lt;- range(pilot$Petal.Length) X_pred &lt;- seq(my_range[1], my_range[2], length = 12) yhat_conf &lt;- data.frame( predict(pilot_mod, interval = &quot;confidence&quot;, newdata = data.frame(Petal.Length = X_pred))) names(yhat_conf) &lt;- c(&quot;yhat_conf_fit&quot;, &quot;yhat_conf_lwr&quot;, &quot;yhat_conf_upr&quot;) yhat_pred &lt;- data.frame( predict(pilot_mod, interval = &quot;prediction&quot;, newdata = data.frame(Petal.Length = X_pred))) names(yhat_pred) &lt;- c(&quot;yhat_pred_fit&quot;, &quot;yhat_pred_lwr&quot;, &quot;yhat_pred_upr&quot;) yhat_all &lt;- data.frame(Petal.Length = X_pred, yhat_conf, yhat_pred[ , -1]) names(yhat_all)[names(yhat_all) %in% &quot;yhat_conf_fit&quot;] &lt;- &quot;Sepal.Length&quot; ## Add to sample data pilot$X_pred &lt;- X_pred pilot &lt;- data.frame(pilot, yhat_conf) pilot &lt;- data.frame(pilot, yhat_pred_lwr = yhat_pred$yhat_pred_lwr, yhat_pred_upr = yhat_pred$yhat_pred_upr) ## Plot fitted values and intervals from &quot;predict&quot; function fit_conf &lt;- ggplot(pilot, aes(x = Petal.Length, y = Sepal.Length)) + geom_point() + geom_smooth( data = yhat_all, aes(ymin = yhat_conf_lwr, ymax = yhat_conf_upr), stat = &quot;identity&quot;) + ggtitle(&quot;Fitted value with 95 % confidence interval (from &#39;predict&#39;)&quot;)+ theme(plot.title = element_text(size = 8, face = &quot;bold&quot;)) fit_pred &lt;- ggplot(pilot, aes(x = Petal.Length, y = Sepal.Length)) + geom_point() + geom_smooth(data = yhat_all, aes(ymin = yhat_pred_lwr, ymax = yhat_pred_upr), stat = &quot;identity&quot;) + ggtitle(&quot;Fitted value with 95 % prediction interval (from &#39;predict&#39;)&quot;)+ theme(plot.title = element_text(size = 8, face = &quot;bold&quot;)) ggarrange(fit_gg, fit_conf, fit_pred, ncol = 3) ## `geom_smooth()` using formula &#39;y ~ x&#39; 2.4 Histogram 2.4.1 Histogram: one vector of values ## Histogram of one vector of values in ggplot2 ## Generate a vector of values s2 &lt;- numeric(5000) for(i in 1:5000) { dd &lt;- rnorm(5, 10, 1) s2[i] &lt;- var(dd) rm(dd) } summary(s2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.009259 0.477847 0.815363 0.990143 1.333825 5.791142 Data in numeric vector ## Data in numeric vector ggplot() + aes(s2) + geom_histogram(binwidth = 0.5) Data in single-column data frame - Y axis is frequency ## Data in single-column data frame s2.df &lt;- data.frame(var = s2) ## Y axis is frequency ggplot(s2.df, aes(x = var)) + geom_histogram(breaks = 0:16*0.5, col = &quot;black&quot;, fill = &quot;grey70&quot;) Data in single-column data frame - Y axis is density ## Y axis is density ggplot(s2.df, aes(x = var, y = ..density..)) + geom_histogram(breaks = 0:16*0.5, col = &quot;black&quot;, fill = &quot;grey70&quot;) For comparison, histogram in lattice ## Lattice histogram(s2, breaks = 0:16*0.5, right = F, main = &quot;Lattice&quot;) range(s2) ## [1] 0.009259423 5.791142367 2.5 Density plot ## Density plot ggplot(s2.df, aes(x = var)) + geom_density() For comparison, density plot in base graphics and in lattice ## Density plot - base graphics plot(density(s2), main = &quot;Base graphics&quot;) ## Density plot - lattice densityplot(s2, plot.points = F, main = &quot;Lattice&quot;) 2.6 Plot structure ggplot_build(p_obj)$data 2.7 Label legend guides(fill = guide_legend(title = &quot;Gender&quot;)) 2.8 Plot title theme(plot.title = element_text(size = 11)) 2.9 Three-category colours scale_fill_manual(values = c(&quot;#f8766d&quot;, &quot;#00bfc4&quot;, &quot;#b79f00&quot;) 2.10 Set breaks for scale scale_y_continuous(breaks = seq(0, 12, by = 2)) 2.11 Display table to 2 d.p. mutate_if(is.numeric, ~ round(., 1)) %&gt;% kable() 2.12 Redefine factor levels for an individual plot nt_youth_all_indiv %&gt;% mutate(Race.nat = factor( Race.nat, levels = c(&quot;CALD&quot;, &quot;Indigenous&quot;, &quot;Caucasian&quot;, &quot;Not recorded&quot;))) %&gt;% group_by(Care_status, Race.nat) %&gt;% summarise(n_indiv = n_distinct(MP_id)) %&gt;% ggplot( etc. ) 2.13 Plotting two data sets of different length on one plot https://gist.github.com/romunov/47f51ccdfe2362c66e60743849fde6b0 xy1 &lt;- data.frame(x = rnorm(10), y = rnorm(10)) xy2 &lt;- data.frame(x = rnorm(5), y = rnorm(5)) library(ggplot2) ggplot() + theme_bw() + geom_point(data = xy1, aes(x = x, y = y)) + geom_point(data = xy2, aes(x = x, y = y), color = &quot;red&quot;) ggplot(xy1, aes(x = x, y = y)) + theme_bw() + geom_point() + # values mapped to aes() in ggplot() call geom_point(data = xy2, color = &quot;red&quot;) # values maped to the same aes() call as above ggplot(xy1, aes(x = x, y = y)) + theme_bw() + geom_point() + # values mapped to aes() in ggplot() call geom_point(data = xy2, aes(x = x, y = y)) # explicit mapping, can be different for this geom ## PGS&#39; extra ggplot(xy1, aes(x = x, y = y)) + theme_bw() + geom_point() + # values mapped to aes() in ggplot() call geom_point( data = xy2, aes(x = x, y = y), colour = &quot;green&quot;) # explicit mapping, can be different for this geom "],["r-markdown.html", "3 R Markdown 3.1 ioslides: Footnotes 3.2 ioslides: Speakerâs notes 3.3 How to make multi-line tables", " 3 R Markdown 3.1 ioslides: Footnotes Add this after yaml header &lt;style&gt; div.footnotes { position: absolute; bottom: 0; margin-bottom: 10px; width: 80%; font-size: 0.6em; } &lt;/style&gt; &lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; $(document).ready(function() { $(&#39;slide:not(.backdrop):not(.title-slide)&#39;).append(&#39;&lt;div class=\\&quot;footnotes\\&quot;&gt;&#39;); $(&#39;footnote&#39;).each(function(index) { var text = $(this).html(); var fnNum = (index+1).toString().sup(); $(this).html(text + fnNum); var footnote = fnNum + &#39;: &#39; + $(this).attr(&#39;content&#39;) + &#39;&lt;br/&gt;&#39;; var oldContent = $(this).parents(&#39;slide&#39;).children(&#39;div.footnotes&#39;).html(); var newContent = oldContent + footnote; $(this).parents(&#39;slide&#39;).children(&#39;div.footnotes&#39;).html(newContent); }); }); &lt;/script&gt; Example footnote: &lt;footnote content = &quot;Dalgaard 2008, Introductory Statistics with R, Springer; OâNeill et al. 1983, Am. Rev. Respir. Dis., 128:1051â1054.&quot;&gt;&lt;/footnote&gt; 3.2 ioslides: Speakerâs notes Put notes in an html block: &lt;div class=&quot;notes&quot;&gt; This is my *note*. - It can contain markdown - like this list &lt;/div&gt; Add ?presentme=true to the link for the slides file Open the file with the augmented link (Chrome works, Safari disables pop-ups). Press âpâ in the top window. Run the slides from the top window, with the other window as the display window. 3.3 How to make multi-line tables Some notes on smoothing functions These notes come from Faraway, Extending the Linear Model with R, p.Â 302 and Wood, GAMs, 1st ed., pp.Â 124, 128, 148-149, 152-154. Using smoothing functions involves a trade-off between model fit and smoothness. If the model is, \\(y_i = f(x_i) + \\epsilon_i\\), one simple criterion that could be used is: \\[ \\frac{1}{n} \\sum (Y_i - f(x_i))^2 + \\lambda \\int [f&#39;&#39;(x)^2] dx \\; , \\] where \\(\\lambda &gt; 0\\) is the smoothing parameter and \\(\\int [f&#39;&#39;(x)^2] dx\\) is a roughness penalty. This model joins the dots and will be too rough, though, but it illustrates the basic idea behind smoothing. It is the smoothing spline in the table below. Type Knots Notes Natural cubic spline Smoothing spline One knot at each data point Smoothest possible interpolator; wasteful Cubic regression spline Penalised regression Choose where to put knots Use a basis for smaller data set than the one to be analysed spline (i.e.Â number of knots much smaller than sample size); still must choose knot locations P-splines Usually evenly spaced Use a B-spline basis; awkward if uneven knot spacing needed Thin plate splines Positions determined from mathematical form of An ideal smoother; construction of thin plate spline model determines balance between fit and smoothness and best function to achieve required smoothing Disadvantages of the above types of basis functions: need to choose knot locations - this introduces extra subjectivity into modelling process can only be used with one predictor variable not clear which basis is better or worse than any other basis "],["tidyverse.html", "4 Tidyverse 4.1 Stacking and unstacking data", " 4 Tidyverse 4.1 Stacking and unstacking data Stack a two-column data frame with pivot_longer and then unstack it with pivot_wider. Firstly with stack and unstack dd1 &lt;- as.data.frame(matrix(rpois(10, 5), nrow = 5)) dd1 ## V1 V2 ## 1 8 5 ## 2 5 3 ## 3 4 3 ## 4 3 2 ## 5 6 8 dd2s &lt;- stack(dd1) dd2s ## values ind ## 1 8 V1 ## 2 5 V1 ## 3 4 V1 ## 4 3 V1 ## 5 6 V1 ## 6 5 V2 ## 7 3 V2 ## 8 3 V2 ## 9 2 V2 ## 10 8 V2 unstack(dd2s) ## V1 V2 ## 1 8 5 ## 2 5 3 ## 3 4 3 ## 4 3 2 ## 5 6 8 Now with pivot_longer and pivot_wider. library(tidyverse) ## ââ Attaching packages âââââââââââââââââââââââââââââ tidyverse 1.3.1 ââ ## â tibble 3.1.2 â purrr 0.3.4 ## â tidyr 1.1.3 â stringr 1.4.0 ## â readr 2.0.0 â forcats 0.5.1 ## ââ Conflicts ââââââââââââââââââââââââââââââââ tidyverse_conflicts() ââ ## x dplyr::collapse() masks nlme::collapse() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x dplyr::select() masks MASS::select() dd2pl &lt;- dd1 %&gt;% pivot_longer(cols = c(&quot;V1&quot;, &quot;V2&quot;)) dd2pl ## # A tibble: 10 x 2 ## name value ## &lt;chr&gt; &lt;int&gt; ## 1 V1 8 ## 2 V2 5 ## 3 V1 5 ## 4 V2 3 ## 5 V1 4 ## 6 V2 3 ## 7 V1 3 ## 8 V2 2 ## 9 V1 6 ## 10 V2 8 dd3pl &lt;- dd2pl %&gt;% group_by(name) %&gt;% mutate(row = row_number() ) %&gt;% tidyr::pivot_wider(names_from = name, values_from = value) %&gt;% select(-row) dd3pl ## # A tibble: 5 x 2 ## V1 V2 ## &lt;int&gt; &lt;int&gt; ## 1 8 5 ## 2 5 3 ## 3 4 3 ## 4 3 2 ## 5 6 8 "],["linear-models.html", "5 Linear Models 5.1 ANOVA: Orthogonal factors 5.2 Multiple linear regression - conditional and marginal effects 5.3 Linear model with subsampling 5.4 Marginal means - emmeans example 5.5 Multiple regression example 5.6 Residual plots - redres 5.7 Interactions - examples 5.8 Interaction plot in ggplot2 5.9 Plot simple linear regression fitted line with 95 % confidence band", " 5 Linear Models 5.1 ANOVA: Orthogonal factors Example from Meier, ANOVA (web document) ## Create data (skip if not interested) #### acids &lt;- c(1.697, 1.601, 1.830, 2.032, 2.017, 2.409, 2.211, 1.673, 1.973, 2.091, 2.255, 2.987) R50 &lt;- rep(c(&quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;), each = 3) R21 &lt;- rep(c(&quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;), each = 3) cheddar &lt;- data.frame(R50, R21, acids) str(cheddar) ## &#39;data.frame&#39;: 12 obs. of 3 variables: ## $ R50 : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;yes&quot; ... ## $ R21 : chr &quot;no&quot; &quot;no&quot; &quot;no&quot; &quot;no&quot; ... ## $ acids: num 1.7 1.6 1.83 2.03 2.02 ... cheddar ## R50 R21 acids ## 1 no no 1.697 ## 2 no no 1.601 ## 3 no no 1.830 ## 4 yes no 2.032 ## 5 yes no 2.017 ## 6 yes no 2.409 ## 7 no yes 2.211 ## 8 no yes 1.673 ## 9 no yes 1.973 ## 10 yes yes 2.091 ## 11 yes yes 2.255 ## 12 yes yes 2.987 table(cheddar$R21, cheddar$R50) ## ## no yes ## no 3 3 ## yes 3 3 Get model matrix for two-factor model cheddar.lm1 &lt;- lm(acids ~ R21 + R50) anova(cheddar.lm1) ## Analysis of Variance Table ## ## Response: acids ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## R21 1 0.21440 0.21440 2.6526 0.13782 ## R50 1 0.65614 0.65614 8.1178 0.01911 * ## Residuals 9 0.72744 0.08083 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 cheddar.lm2 &lt;- lm(acids ~ R50 + R21) anova(cheddar.lm2) ## Analysis of Variance Table ## ## Response: acids ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## R50 1 0.65614 0.65614 8.1178 0.01911 * ## R21 1 0.21440 0.21440 2.6526 0.13782 ## Residuals 9 0.72744 0.08083 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Sums of squares are the same regardless of the order of R21 and R50 in the model. Get model matrix. X &lt;- model.matrix(cheddar.lm1) X ## (Intercept) R21yes R50yes ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 1 ## 5 1 0 1 ## 6 1 0 1 ## 7 1 1 0 ## 8 1 1 0 ## 9 1 1 0 ## 10 1 1 1 ## 11 1 1 1 ## 12 1 1 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$R21 ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$R50 ## [1] &quot;contr.treatment&quot; Calculate \\(\\textbf{X}&#39;\\textbf{X}\\) t(X) %*% X ## (Intercept) R21yes R50yes ## (Intercept) 12 6 6 ## R21yes 6 6 3 ## R50yes 6 3 6 solve(t(X) %*% X) ## (Intercept) R21yes R50yes ## (Intercept) 0.2500000 -0.1666667 -0.1666667 ## R21yes -0.1666667 0.3333333 0.0000000 ## R50yes -0.1666667 0.0000000 0.3333333 5.2 Multiple linear regression - conditional and marginal effects Note: See Davidâs book, ch.Â 3. library(tidyverse) data_height &lt;- read_csv(&quot;data/plantHeightSingleSpp.csv&quot;) ## Rows: 178 Columns: 35 ## ââ Column specification ââââââââââââââââââââââââââââââââââââââââââââââ ## Delimiter: &quot;,&quot; ## chr (6): Genus_species, Family, growthform, Country, Site, entere... ## dbl (29): sort_number, site, height, loght, lat, long, alt, temp, ... ## ## â¹ Use `spec()` to retrieve the full column specification for this data. ## â¹ Specify the column types or set `show_col_types = FALSE` to quiet this message. mod_height1 &lt;- lm(height ~ lat, data = data_height) summary(mod_height1) ## ## Call: ## lm(formula = height ~ lat, data = data_height) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.558 -6.955 -3.978 3.654 54.333 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.42516 1.72433 8.366 1.78e-14 *** ## lat -0.17631 0.04847 -3.637 0.000362 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.95 on 176 degrees of freedom ## Multiple R-squared: 0.06991, Adjusted R-squared: 0.06463 ## F-statistic: 13.23 on 1 and 176 DF, p-value: 0.0003617 mod_height2 &lt;- lm(height ~ rain + lat, data = data_height) summary(mod_height2) ## ## Call: ## lm(formula = height ~ rain + lat, data = data_height) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.738 -5.932 -3.418 2.833 52.188 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.521724 3.077683 1.469 0.143575 ## rain 0.004058 0.001062 3.823 0.000183 *** ## lat -0.034109 0.059707 -0.571 0.568547 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.55 on 175 degrees of freedom ## Multiple R-squared: 0.1416, Adjusted R-squared: 0.1318 ## F-statistic: 14.43 on 2 and 175 DF, p-value: 1.579e-06 Reverse order of model terms mod_height2a &lt;- lm(height ~ lat + rain, data = data_height) summary(mod_height2a) ## ## Call: ## lm(formula = height ~ lat + rain, data = data_height) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.738 -5.932 -3.418 2.833 52.188 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.521724 3.077683 1.469 0.143575 ## lat -0.034109 0.059707 -0.571 0.568547 ## rain 0.004058 0.001062 3.823 0.000183 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.55 on 175 degrees of freedom ## Multiple R-squared: 0.1416, Adjusted R-squared: 0.1318 ## F-statistic: 14.43 on 2 and 175 DF, p-value: 1.579e-06 drop1(mod_height2, test = &quot;F&quot;) ## Single term deletions ## ## Model: ## height ~ rain + lat ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 19469 841.67 ## rain 1 1625.55 21094 853.95 14.6117 0.0001834 *** ## lat 1 36.31 19505 840.00 0.3264 0.5685469 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.2.1 Marginal and Conditional interpretations The marginal effect of a predictor variable in a regression model is the effect of that variable alone on the outcome, estimated without including any other predictor variable in the model. The model above with lat as the only predictor variable, mod_height1, gives the marginal effect of lat as -0.176. The conditional effect of a predictor variable in a regression model is the effect of that variable on the outcome when other predictors are included in the model and all those other predictors are held constant. The model above with both lat and rain as predictor variables, mod_height2, gives the conditional effect of lat, after controlling for the effect of rain, as -0.034. Note that in R, the âsummaryâ output for âlmâ gives the conditional effect of each predictor variable - i.e.Â the effect conditional on all the other predictor variables, or in other words, after adjusting for or controlling for, all the other predictor variables. This is shown by comparing the three blocks of output immediately above given by summary(mod_height2), summary(mod_height2a) and drop1(mod_height2). 5.3 Linear model with subsampling 5.3.1 Load necessary R packages library(gamair) ## Data library(ggplot2) library(nlme) library(emmeans) library(dplyr) Example Note: This example is from the book, Wood, S. 2017. Generalized Additive Models: An Introduction with R. CRC Press, 2nd ed., pp.Â 61-65. This book is available as an e-book in UNSW library. (The example is also in the 1st edition of this book on pp.Â 277-281. Another example of this idea with a different data set, but without the mixed model analysis, is in Steel, R., Torrie, J. and Dickey, D. 1997. Principles and Procedures of Statistics: A Biometrical Approach. McGraw Hill, 3rd ed., pp.Â 157-165.) Tree seedlings are grown under two levels of carbon dioxide concentration, with three trees assigned to each treatment. At six months of growth, stomatal area is measured at each of four random locations on each plant. Notes âStomatesâ are the tiny holes in plant leaves through which carbon dioxide and water vapour are exchanged with the atmosphere during photosynthesis and respiration. The sample sizes here are artificially small simply to illustrate the ideas. 5.3.2 Data The data, âstomataâ, is included in the R package, âgamairâ. data(stomata) str(stomata) ## &#39;data.frame&#39;: 24 obs. of 3 variables: ## $ area: num 1.61 1.63 1.54 1.72 1.39 ... ## $ CO2 : Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ tree: Factor w/ 6 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... head(stomata) ## area CO2 tree ## 1 1.605574 1 1 ## 2 1.630071 1 1 ## 3 1.539119 1 1 ## 4 1.718732 1 1 ## 5 1.389616 1 2 ## 6 1.585880 1 2 Convert CO2 and tree to factors. stomata$CO2 &lt;- factor(stomata$CO2) ## For CO2, &quot;1&quot; is &quot;low&quot; and &quot;2&quot; is &quot;high&quot;. ## Add these labels. stomata$CO2 &lt;- factor(stomata$CO2, labels = c(&quot;low&quot;, &quot;high&quot;)) stomata$tree &lt;- factor(stomata$tree) 5.3.3 Summary statistics and plots summary(stomata) ## area CO2 tree ## Min. :0.8753 low :12 1:4 ## 1st Qu.:1.5396 high:12 2:4 ## Median :2.0166 3:4 ## Mean :2.0679 4:4 ## 3rd Qu.:2.7600 5:4 ## Max. :3.1149 6:4 boxplot(stomata$area ~ stomata$CO2) Plot with a strip plot (strip chart) as sample sizes are small. stripchart(area ~ CO2, data = stomata, ylab = &quot;CO2 conc.&quot;) ggplot(stomata, aes(x = CO2, y = area, colour = tree)) + geom_jitter(position = position_jitter(0.075)) The variance appears fairly constant and the distribution looks fairly symmetric. (This suggests the linear model assumptions will be met, but theyâll be checked later. The purpose of this example is to illustrate the proper way to analyse data from this type of experimental design and not to worry about assumptions, though.) Note: Where youâve got data with a quantitative, continuous response variable, the usual function to use for a linear model is âlmâ. In the short course, Introductory Statistics for Researchers, we used a function, âaovâ, for the one-way ANOVA model we fitted. That function is fairly limited in when you can use it and so âlmâ is the function most commonly used. However, there are situations where lm is not appropriate. The following analyses are taken from the book by Simon Wood cited above. Explanations of whatâs going on are more detailed in the book. Itâs useful to read the material in the book about this example but not to worry about the technical details there. 5.3.4 Wrong analysis: fixed effects linear model To account for stomatal area here, weâd expect some contribution from CO2 concentration (there are two levels, low and high) and some contribution from individual trees. The following analysis looks reasonable at face value. m1 &lt;- lm(area ~ CO2 + tree, data = stomata) summary(m1) ## ## Call: ## lm(formula = area ~ CO2 + tree, data = stomata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.30672 -0.10625 -0.01528 0.08436 0.37674 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.62337 0.10932 14.850 1.52e-11 *** ## CO2high 0.70639 0.15460 4.569 0.000238 *** ## tree2 -0.02473 0.15460 -0.160 0.874685 ## tree3 -0.46041 0.15460 -2.978 0.008059 ** ## tree4 0.45948 0.15460 2.972 0.008166 ** ## tree5 0.57378 0.15460 3.711 0.001597 ** ## tree6 NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2186 on 18 degrees of freedom ## Multiple R-squared: 0.9215, Adjusted R-squared: 0.8997 ## F-statistic: 42.24 on 5 and 18 DF, p-value: 2.511e-09 From above, \\(P &lt; 0.0001\\). The âNAsâ in the output look odd. The above model is wrong, however, because CO2 and tree are confounded. Trees are nested in CO2 treatments - we cannot, for example, separate the effect of low CO2 from the effect of tree 1 on the predicted value of area for that tree. Compare the above model with a model including only CO2. The âanovaâ function below compares the two models - the \\(P\\) value in the output is testing whether the two models are no better than each other. m0 &lt;- lm(area ~ CO2, data = stomata) anova(m0, m1) ## Analysis of Variance Table ## ## Model 1: area ~ CO2 ## Model 2: area ~ CO2 + tree ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 22 2.1348 ## 2 18 0.8604 4 1.2744 6.6654 0.001788 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The above output says that the models are not equally good ( \\(P = 0.002\\), the \\(P\\) value is very small); consequently, the larger model is better - with more explanatory variables, it has more explanatory value. As model m0 contains only CO2 and m1 contains both CO2 and tree, then tree differences are important. However, from those models, we have no way of testing whether CO2 differences are important (as the only difference between the models was whether they included tree or not). Test for CO2 effects by comparing models with tree but with and without CO2. m2 &lt;- lm(area ~ tree, data = stomata) anova(m2, m1) ## Analysis of Variance Table ## ## Model 1: area ~ tree ## Model 2: area ~ CO2 + tree ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 18 0.8604 ## 2 18 0.8604 0 2.2204e-16 The above output shows the two models are the same (the RSS, residual sum of squares, for each is the same) and so we cannot use this analysis to test for CO2 effects. This is caused by the confounding problem. One way of getting around this problem is to average the response variable values for each tree and use the averaged values as our response variable. Note you can only do this if there are the same number of data values in each group you average and there are no missing values! stomata.avg &lt;- stomata %&gt;% group_by(tree) %&gt;% summarise(area = mean(area)) CO2_by_tree &lt;- stomata %&gt;% distinct(tree, CO2) stomata.avg &lt;- data.frame(stomata.avg, CO2 = CO2_by_tree$CO2) str(stomata.avg) ## &#39;data.frame&#39;: 6 obs. of 3 variables: ## $ tree: Factor w/ 6 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 ## $ area: num 1.62 1.6 1.16 2.79 2.9 ... ## $ CO2 : Factor w/ 2 levels &quot;low&quot;,&quot;high&quot;: 1 1 1 2 2 2 m3 &lt;- lm(area ~ CO2, data = stomata.avg) summary(m3) ## ## Call: ## lm(formula = area ~ CO2, data = stomata.avg) ## ## Residuals: ## 1 2 3 4 5 6 ## 0.1617 0.1370 -0.2987 0.1151 0.2294 -0.3444 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.4617 0.1629 8.970 0.000855 *** ## CO2high 1.2125 0.2304 5.262 0.006247 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2822 on 4 degrees of freedom ## Multiple R-squared: 0.8738, Adjusted R-squared: 0.8422 ## F-statistic: 27.69 on 1 and 4 DF, p-value: 0.006247 anova(m3) ## Analysis of Variance Table ## ## Response: area ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## CO2 1 2.20531 2.20531 27.687 0.006247 ** ## Residuals 4 0.31861 0.07965 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The above analysis is valid, although it does involve a two-step process where you average the data for each tree first. Another way of getting around this problem is to use the âaovâ function here. Note the same conditions in bold above apply to using the aov function. We use the original, non-averaged data with aov. m4 &lt;- aov(area ~ CO2 + Error(tree), data = stomata) summary(m4) ## ## Error: tree ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## CO2 1 8.821 8.821 27.69 0.00625 ** ## Residuals 4 1.274 0.319 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 18 0.8604 0.0478 From the above two analyses, the correct \\(P\\) value for the effect of CO2 is 0.00625. 5.3.5 Right analysis: mixed effects linear model A better (and correct) approach is to use a mixed effects model. This works even if treatment groups do not have the same number of replicates (or if there are missing values). In this approach, tree is included as a grouping variable to identify clusters of data values (i.e.Â to identify which observed values come from which tree). Observations from the same tree will be correlated to some extent and this needs to be incorporated in the model via a grouping variable. If these groups were not specified in the model, it would appear as if all observations were independent and this would lead to smaller uncertainty in the estimates of treatment effects (because youâd be overstaing the number of independent observations, and hence, the amount of information you really had, or âinformation valueâ of your data). The trees in the study are considered as being randomly sampled from the (large) population of trees we are studying. The effects of individual trees are not considered important but they need to be included in the model to identify groups of data values. So, letâs fit a mixed model. You can use the ânlmeâ package or the âlme4â package. The nlme package is used here because it gives \\(P\\) values, while the lme4 package does not (another story!). m5 &lt;- lme(area ~ CO2, random = ~ 1 | tree, data = stomata) #summary(m5) summary(m5)$tTable ## Value Std.Error DF t-value p-value ## (Intercept) 1.461659 0.1629435 18 8.970341 4.625996e-08 ## CO2high 1.212522 0.2304370 4 5.261837 6.246688e-03 From the above output, the \\(P\\) value for the effect of CO2 concentration is 0.00625. This is the same as the \\(P\\) values above using the averaged data and using the aov function. 5.3.6 An aside - checking model assumptions Letâs check the assumptions for the mixed model because we can. This shows how to check the two assumptions of constant variance and Normal distribution for a linear mixed model. We wonât worry here about whether the assumptions are satisfied, as the emphasis is on appropriate methods to use. Equal variance assumption Use the plot function with the model name. This produces a residuals vs fitted values plot, which is the standard plot for checking the constant variance assumption. plot(m5) The variances appear to be somewhat unequal. Normal distribution assumption Use a Normal quantile-quantile (Q-Q) plot. qqnorm(residuals(m5)) qqline(residuals(m5)) The Normal distribution assumption does not appear to be particularly well met. Back to the main story â¦ With balanced data, youâll get the same results from an analysis using aov or lme. However, itâs preferable to use a mixed effects model, as recommended by Simon Wood and described here. 5.3.7 Fitted values and confidence intervals The R package, âemmeansâ, is very useful for getting means and their confidence intervals and plots of these. This package will work with models from lm, aov , lme, â¦. Estimates and plots from the fixed effects model using averaged data and the mixed model are below. m3.emm &lt;- emmeans(m3, ~ CO2) summary(m3.emm) ## CO2 emmean SE df lower.CL upper.CL ## low 1.46 0.163 4 1.01 1.91 ## high 2.67 0.163 4 2.22 3.13 ## ## Confidence level used: 0.95 plot(m3.emm) m5.emm &lt;- emmeans(m5, ~CO2, mode = &quot;satterthwaite&quot;) summary(m5.emm) ## CO2 emmean SE df lower.CL upper.CL ## low 1.46 0.163 3.99 1.01 1.91 ## high 2.67 0.163 3.99 2.22 3.13 ## ## Degrees-of-freedom method: satterthwaite ## Confidence level used: 0.95 plot(m5.emm) 5.3.8 Another aside - code the nested structure - not recommended The analysis can be done with lm if the model is coded so that the nesting of trees within CO2 treatments is incorporated. It can only be done this way because, again, the data is balanced and there are no missing values. The full data set is used. m6 &lt;- lm(area ~ CO2 / tree, data = stomata) anova(m6) ## Analysis of Variance Table ## ## Response: area ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## CO2 1 8.8213 8.8213 184.5452 6.686e-11 *** ## CO2:tree 4 1.2744 0.3186 6.6654 0.001788 ** ## Residuals 18 0.8604 0.0478 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In the output above, the effect of CO2 needs to be compared against the correct residual term. The \\(P\\) value against CO2 shown in the output is obtained by comparing the variation of CO2 treatments against the variation among observations within trees. This is not the correct comparison. As tree is the experimental unit (leaf is the observational unit), the correct term for the comparison is the term corresponding to the variation between trees within CO2 treatments. In the output, that term is CO2:tree. Consequently, the correct \\(P\\) value for the effect of CO2 is obtained from the \\(F\\) statistic calculated as 8.8213 / 0.3186 (i.e.Â (Mean Sq for CO2) / (Mean Sq for appropriate residual term)). The calculation for obtaining the \\(P\\) value is below. f_obs &lt;- 8.8213 / 0.3186 f_obs ## [1] 27.6877 This is the correct \\(F\\) value, as seen previously. p_val &lt;- 1 - pf(f_obs, 1, 4) p_val ## [1] 0.00624638 The \\(P\\) value is also clearly the same as seen previously. It could be awkward to get correct confidence intervals from this model. 5.3.9 Summary When you have subsampling (i.e.Â multiple measurements made on individual experimental units), use a mixed effects model. If you have unequal replication across treatments or missing values, definitely use a mixed effects model. 5.4 Marginal means - emmeans example library(emmeans) library(tidyr) library(dplyr) library(RcmdrMisc) ## Loading required package: car ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## some ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:boot&#39;: ## ## logit ## Loading required package: sandwich From the document, Basics of estimated marginal means. https://cran.r-project.org/web/packages/emmeans/vignettes/basics.html pigs {emmeans} R Documentation Effects of dietary protein on free plasma leucine concentration in pigs Description A two-factor experiment with some observations lost A data frame with 29 observations and 3 variables: source: Source of protein in the diet (factor with 3 levels: fish meal, soybean meal, dried skim milk) percent: Protein percentage in the diet (numeric with 4 values: 9, 12, 15, and 18) conc: Concentration of free plasma leucine, in mcg/ml Source: Windels HF (1964) PhD thesis, Univ. of Minnesota. (Reported as Problem 10.8 in Oehlert G (2000) A First Course in Design and Analysis of Experiments, licensed under Creative Commons, http://users.stat.umn.edu/~gary/Book.html.) Observations 7, 22, 23, 31, 33, and 35 have been omitted, creating a more notable imbalance. [Package emmeans version 1.4.6] data(pigs, package = &quot;emmeans&quot;) pigs ## source percent conc ## 1 fish 9 27.8 ## 2 fish 9 23.7 ## 3 fish 12 31.5 ## 4 fish 12 28.5 ## 5 fish 12 32.8 ## 6 fish 15 34.0 ## 7 fish 15 28.3 ## 8 fish 18 30.6 ## 9 fish 18 32.7 ## 10 fish 18 33.7 ## 11 soy 9 39.3 ## 12 soy 9 34.8 ## 13 soy 9 29.8 ## 14 soy 12 39.8 ## 15 soy 12 40.0 ## 16 soy 12 39.1 ## 17 soy 15 38.5 ## 18 soy 15 39.2 ## 19 soy 15 40.0 ## 20 soy 18 42.9 ## 21 skim 9 40.6 ## 22 skim 9 31.0 ## 23 skim 9 34.6 ## 24 skim 12 42.9 ## 25 skim 12 50.1 ## 26 skim 12 37.4 ## 27 skim 15 59.5 ## 28 skim 15 41.4 ## 29 skim 18 59.8 pigs$percent &lt;- factor(pigs$percent) numSummary(pigs[ , 3], statistics = &quot;mean&quot;, groups = interaction(pigs$source, pigs$percent)) ## mean n ## fish.9 25.75000 2 ## soy.9 34.63333 3 ## skim.9 35.40000 3 ## fish.12 30.93333 3 ## soy.12 39.63333 3 ## skim.12 43.46667 3 ## fish.15 31.15000 2 ## soy.15 39.23333 3 ## skim.15 50.45000 2 ## fish.18 32.33333 3 ## soy.18 42.90000 1 ## skim.18 59.80000 1 Cell means (see also below) These are arithmetic means of values in each two-way combination with(pigs, tapply(conc, INDEX = list(source, percent), mean)) ## 9 12 15 18 ## fish 25.75000 30.93333 31.15000 32.33333 ## soy 34.63333 39.63333 39.23333 42.90000 ## skim 35.40000 43.46667 50.45000 59.80000 with(pigs, interaction.plot(percent, source, conc)) Marginal means Arithmetic mean of values in each âpercentâ group with(pigs, tapply(conc, percent, mean)) ## 9 12 15 18 ## 32.70000 38.01111 40.12857 39.94000 Find average for percent = 15 ( (34.0 + 28.3) + (38.5 + 39.2 + 40.0) + (59.5 + 41.4) ) / 7 ## [1] 40.12857 Cell means Arithmetic means of values in each combination - same as above with two-way tapply call cell.means &lt;- matrix(with(pigs, tapply(conc, interaction(source, percent), mean)), nrow = 3) cell.means ## [,1] [,2] [,3] [,4] ## [1,] 25.75000 30.93333 31.15000 32.33333 ## [2,] 34.63333 39.63333 39.23333 42.90000 ## [3,] 35.40000 43.46667 50.45000 59.80000 apply(cell.means, 2, mean) ## [1] 31.92778 38.01111 40.27778 45.01111 Compare with arithmetic mean of values in each âpercentâ group with(pigs, tapply(conc, percent, mean)) ## 9 12 15 18 ## 32.70000 38.01111 40.12857 39.94000 The two sets are different because the different cells do not all have the same number of observations (some observations were lost). with(pigs, table(source, percent)) ## percent ## source 9 12 15 18 ## fish 2 3 2 3 ## soy 3 3 3 1 ## skim 3 3 2 1 The marginal mean for percent = 12 is the same as the average of the cell means because no observations were lost - i.e.Â there are equal weights on all the cell means. We can reproduce the marginal means by weighting the cell means with these frequencies. For example, in the last column: sum(c(3, 1, 1) * cell.means[ , 4]) / 5 ## [1] 39.94 5.5 Multiple regression example library(dplyr) library(ggplot2) library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ## ## Attaching package: &#39;GGally&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## pigs ## The following object is masked from &#39;package:emmeans&#39;: ## ## pigs library(sjPlot) ## Registered S3 methods overwritten by &#39;parameters&#39;: ## method from ## as.double.parameters_kurtosis datawizard ## as.double.parameters_skewness datawizard ## as.double.parameters_smoothness datawizard ## as.numeric.parameters_kurtosis datawizard ## as.numeric.parameters_skewness datawizard ## as.numeric.parameters_smoothness datawizard ## print.parameters_distribution datawizard ## print.parameters_kurtosis datawizard ## print.parameters_skewness datawizard ## summary.parameters_kurtosis datawizard ## summary.parameters_skewness datawizard library(ggpubr) library(emmeans) Example from StatSci.org - OzDASL, Australian Institute of Sport http://www.statsci.org/data/oz/ais.html Variable Description Sport Sport Sex male or female Ht Height in cm Wt Weight in kg LBM Lean body mass RCC Red cell count WCC White cell count Hc Hematocrit Hg Hemoglobin Ferr Plasma ferritin concentration BMI Body mass index = weight/height^2 SSF Sum of skin folds %Bfat % body fat athletes &lt;- read.table(&quot;http://www.statsci.org/data/oz/ais.txt&quot;, sep = &quot;&quot;, header = T) str(athletes) save(athletes, file = &quot;athletes_OzDASL.RData&quot;) load(&quot;data/athletes_OzDASL.Rdata&quot;) str(athletes) ## &#39;data.frame&#39;: 202 obs. of 13 variables: ## $ Sex : chr &quot;female&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ... ## $ Sport : chr &quot;BBall&quot; &quot;BBall&quot; &quot;BBall&quot; &quot;BBall&quot; ... ## $ RCC : num 3.96 4.41 4.14 4.11 4.45 4.1 4.31 4.42 4.3 4.51 ... ## $ WCC : num 7.5 8.3 5 5.3 6.8 4.4 5.3 5.7 8.9 4.4 ... ## $ Hc : num 37.5 38.2 36.4 37.3 41.5 37.4 39.6 39.9 41.1 41.6 ... ## $ Hg : num 12.3 12.7 11.6 12.6 14 12.5 12.8 13.2 13.5 12.7 ... ## $ Ferr : int 60 68 21 69 29 42 73 44 41 44 ... ## $ BMI : num 20.6 20.7 21.9 21.9 19 ... ## $ SSF : num 109.1 102.8 104.6 126.4 80.3 ... ## $ X.Bfat: num 19.8 21.3 19.9 23.7 17.6 ... ## $ LBM : num 63.3 58.5 55.4 57.2 53.2 ... ## $ Ht : num 196 190 178 185 185 ... ## $ Wt : num 78.9 74.4 69.1 74.9 64.6 63.7 75.2 62.3 66.5 62.9 ... athletes$Sex &lt;- factor(athletes$Sex) athletes$Sport &lt;- factor(athletes$Sport) #names( athletes )[names(athletes) == &quot;X.Bfat&quot;] &lt;- &quot;Bfat_pc&quot; ## or athletes &lt;- athletes %&gt;% rename( Bfat_pc = X.Bfat) 5.5.1 Regression model Response: LBM Predictors: height weight gender Ferr Include log(Ferr) as suggested on OzDASL web page. athletes$log_Ferr &lt;- log(athletes$Ferr) ggpairs(athletes, columns = c(7, 14, 12, 13, 11)) ggplot(athletes, aes(x = Sex, y = LBM)) + geom_boxplot() ath.lm1 &lt;- lm(LBM ~ Ht + Wt +log_Ferr + Sex, data = athletes) summary(ath.lm1) ## ## Call: ## lm(formula = LBM ~ Ht + Wt + log_Ferr + Sex, data = athletes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.9543 -1.5237 0.2123 1.6439 8.8396 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.30242 5.49116 -0.237 0.8128 ## Ht 0.08320 0.03400 2.447 0.0153 * ## Wt 0.65455 0.02326 28.141 &lt;2e-16 *** ## log_Ferr -0.61689 0.35494 -1.738 0.0838 . ## Sexmale 9.23148 0.51135 18.053 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.713 on 197 degrees of freedom ## Multiple R-squared: 0.9578, Adjusted R-squared: 0.9569 ## F-statistic: 1117 on 4 and 197 DF, p-value: &lt; 2.2e-16 ath.lm2 &lt;- lm(LBM ~ Ht , data = athletes) summary(ath.lm2) ## ## Call: ## lm(formula = LBM ~ Ht, data = athletes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.566 -4.913 -0.556 4.171 31.853 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -129.0947 10.2259 -12.62 &lt;2e-16 *** ## Ht 1.0770 0.0567 19.00 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.825 on 200 degrees of freedom ## Multiple R-squared: 0.6434, Adjusted R-squared: 0.6416 ## F-statistic: 360.8 on 1 and 200 DF, p-value: &lt; 2.2e-16 par(mfrow = c(2, 2)) plot(ath.lm1) par(mfrow = c(1, 1)) plot1 &lt;- plot_model(ath.lm1, type = &quot;pred&quot;, terms = &quot;Ht&quot;) + ggtitle(&quot;Predicted values of mean LBM with 95 % confidence band&quot;) + theme(plot.title = element_text(size = 8)) print(plot1) The plot above shows a confidence band for predicted mean LBM, as seen by comparing the intervals below with the output of the âpredictâ function further below. plot1$data ## # Predicted values of LBM ## ## Ht | Predicted | group_col | 95% CI ## -------------------------------------------- ## 140 | 56.88 | 1 | [54.30, 59.45] ## 150 | 57.71 | 1 | [55.78, 59.64] ## 160 | 58.54 | 1 | [57.22, 59.86] ## 170 | 59.37 | 1 | [58.59, 60.16] ## 180 | 60.20 | 1 | [59.58, 60.83] ## 190 | 61.04 | 1 | [60.01, 62.06] ## 200 | 61.87 | 1 | [60.25, 63.48] ## 210 | 62.70 | 1 | [60.45, 64.95] ## ## Adjusted for: ## * Wt = 75.01 ## * log_Ferr = 4.16 ## * Sex = female Confidence band - band for fitted mean X_new &lt;- expand.grid(Ht = 14:21*10, Wt = 75.01, log_Ferr = 4.16, Sex = &quot;female&quot;) ath.lm1.pred.CI &lt;- predict(ath.lm1, newdata = X_new, se.fit = T, type = &quot;response&quot;, interval = &quot;confidence&quot;) data.frame(Ht = 14:21*10, se = ath.lm1.pred.CI$se.fit, ath.lm1.pred.CI$fit) ## Ht se fit lwr upr ## 1 140 1.3130285 56.87714 54.28775 59.46654 ## 2 150 0.9858112 57.70914 55.76505 59.65324 ## 3 160 0.6714313 58.54115 57.21703 59.86526 ## 4 170 0.4012888 59.37315 58.58177 60.16452 ## 5 180 0.3200962 60.20515 59.57389 60.83640 ## 6 190 0.5245063 61.03715 60.00278 62.07152 ## 7 200 0.8239954 61.86915 60.24417 63.49414 ## 8 210 1.1463186 62.70115 60.44052 64.96178 Prediction band - band for single prediction ath.lm1.pred.PI &lt;- predict(ath.lm1, newdata = X_new, se.fit = T, type = &quot;response&quot;, interval = &quot;prediction&quot;) data.frame(Ht = 14:21*10, se = ath.lm1.pred.PI$se.fit, ath.lm1.pred.PI$fit) ## Ht se fit lwr upr ## 1 140 1.3130285 56.87714 50.93353 62.82075 ## 2 150 0.9858112 57.70914 52.01695 63.40134 ## 3 160 0.6714313 58.54115 53.02981 64.05248 ## 4 170 0.4012888 59.37315 53.96502 64.78127 ## 5 180 0.3200962 60.20515 54.81812 65.59217 ## 6 190 0.5245063 61.03715 55.58816 66.48614 ## 7 200 0.8239954 61.86915 56.27790 67.46040 ## 8 210 1.1463186 62.70115 56.89323 68.50908 ggplot(data = athletes, aes(x = Ht, y = LBM)) + geom_point() plot2 &lt;- plot_model(ath.lm1, type = &quot;pred&quot;, terms = &quot;Wt&quot;) + ggtitle(&quot;Predicted values of mean LBM with 95 % confidence band&quot;) + theme(plot.title = element_text(size = 8)) print(plot2) plot3 &lt;- plot_model(ath.lm1, type = &quot;pred&quot;, terms = &quot;log_Ferr&quot;) + ggtitle(&quot;Predicted values of mean LBM with 95 % confidence band&quot;) + theme(plot.title = element_text(size = 8)) print(plot3) plot4 &lt;- plot_model(ath.lm1, type = &quot;pred&quot;, terms = &quot;Sex&quot;) + ggtitle(&quot;Predicted values of mean LBM with 95 % confidence band&quot;) + theme(plot.title = element_text(size = 8)) print(plot4) plot4$data ## # Predicted values of LBM ## ## Sex | Predicted | group_col | 95% CI ## -------------------------------------------- ## 1 | 60.21 | 1 | [59.58, 60.84] ## 2 | 69.44 | 1 | [68.82, 70.07] ## ## Adjusted for: ## * Ht = 180.10 ## * Wt = 75.01 ## * log_Ferr = 4.16 plot_all &lt;- ggarrange(plot1, plot2, plot3, plot4, nrow = 2, ncol = 2) print(plot_all) pdf(file = &quot;athletes_reg_plot_panel.pdf&quot;, width = 7, height = 5) plot_all dev.off() ## quartz_off_screen ## 2 5.6 Residual plots - redres redres package https://goodekat.github.io/redres/index.html 5.7 Interactions - examples Example taken from UCLA IDRE Stata examples. https://stats.idre.ucla.edu/stata/webbooks/reg/chapter7/regressionwith-statachapter-7-more-on-interactions-of-categorical-and-continuous-variables/ Data file is elemapi2.csv. Description of some variables Academic performance of the school (api00), the average class size in kindergarten through 3rd grade (acs_k3), the percentage of students receiving free meals (meals) â which is an indicator of poverty, and the percentage of teachers who have full teaching credentials (full). reg1 &lt;- read.csv(&quot;data/elemapi2.csv&quot;) names(reg1) ## [1] &quot;snum&quot; &quot;dnum&quot; &quot;api00&quot; &quot;api99&quot; &quot;growth&quot; &quot;meals&quot; ## [7] &quot;ell&quot; &quot;yr_rnd&quot; &quot;mobility&quot; &quot;acs_k3&quot; &quot;acs_46&quot; &quot;not_hsg&quot; ## [13] &quot;hsg&quot; &quot;some_col&quot; &quot;col_grad&quot; &quot;grad_sch&quot; &quot;avg_ed&quot; &quot;full&quot; ## [19] &quot;emer&quot; &quot;enroll&quot; &quot;mealcat&quot; &quot;collcat&quot; reg1$collcat &lt;- factor(reg1$collcat) relevel(reg1$collcat, ref = &quot;1&quot;) boxplot(reg1$api00 ~ reg1$collcat, xlab = &quot;collcat&quot;, ylab = &quot;api00&quot;) plot(reg1$api00 ~ reg1$meals, xlab = &quot;meals&quot;, ylab = &quot;api00&quot;) Model with equal slopes for the âcollcatâ level lines api00.lm1 &lt;- lm(api00 ~ collcat + meals, data = reg1) summary(api00.lm1) ## ## Call: ## lm(formula = api00 ~ collcat + meals, data = reg1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -204.742 -43.655 -2.151 39.005 179.714 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 872.42724 8.77485 99.424 &lt; 2e-16 *** ## collcat2 14.01454 7.62786 1.837 0.06692 . ## collcat3 24.24049 7.72798 3.137 0.00184 ** ## meals -3.94267 0.09883 -39.892 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 61.27 on 396 degrees of freedom ## Multiple R-squared: 0.8159, Adjusted R-squared: 0.8145 ## F-statistic: 584.8 on 3 and 396 DF, p-value: &lt; 2.2e-16 par(mfrow = c(2, 2)) plot(api00.lm1) par(mfrow = c(1, 1)) ## Assumptions satisfied summary(reg1$meals) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00 31.00 67.50 60.31 90.00 100.00 newdd &lt;- expand.grid(collcat = c(1:3), meals = seq(from = 0, to = 100)) str(newdd) ## &#39;data.frame&#39;: 303 obs. of 2 variables: ## $ collcat: int 1 2 3 1 2 3 1 2 3 1 ... ## $ meals : int 0 0 0 1 1 1 2 2 2 3 ... ## - attr(*, &quot;out.attrs&quot;)=List of 2 ## ..$ dim : Named int [1:2] 3 101 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;collcat&quot; &quot;meals&quot; ## ..$ dimnames:List of 2 ## .. ..$ collcat: chr [1:3] &quot;collcat=1&quot; &quot;collcat=2&quot; &quot;collcat=3&quot; ## .. ..$ meals : chr [1:101] &quot;meals= 0&quot; &quot;meals= 1&quot; &quot;meals= 2&quot; &quot;meals= 3&quot; ... newdd$collcat &lt;- factor(newdd$collcat) yhat1.api00 &lt;- predict(api00.lm1, newdata = newdd) pred1.plot &lt;- data.frame(cbind(newdd, yhat1 = yhat1.api00)) ## Plot using &quot;xyplot&quot; xyplot(yhat1 ~ meals, groups = collcat, data = pred1.plot, type = &quot;l&quot;, lty = 1:3, auto.key = list(lines = T, points = F, title = &quot;collcat&quot;, space = &quot;top&quot;, columns = 3, cex = 0.5)) ## Plot using base graphics - &quot;plot&quot; with(pred1.plot[pred1.plot$collcat == &quot;1&quot;, ], plot(meals, yhat1, type = &quot;l&quot;, ylim = c(min(pred1.plot$yhat1), max(pred1.plot$yhat1)))) with(pred1.plot[pred1.plot$collcat == &quot;2&quot;, ], lines(meals, yhat1, lty = 2)) with(pred1.plot[pred1.plot$collcat == &quot;3&quot;, ], lines(meals, yhat1, lty = 3)) legend(x = 90, y = 890, legend = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), title = &quot;collcat&quot;, lty = 1:3, cex = 0.6) library(ggplot2) ggplot(pred1.plot, aes(meals, yhat1, group = collcat)) + geom_line(aes(colour = collcat, linetype = collcat)) Model with different slopes for the âcollcatâ level lines api00.lm2 &lt;- lm(api00 ~ collcat * meals, data = reg1) summary(api00.lm2) ## ## Call: ## lm(formula = api00 ~ collcat * meals, data = reg1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -202.159 -41.028 -2.995 37.888 162.297 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 886.13253 12.08177 73.345 &lt; 2e-16 *** ## collcat2 10.29492 16.24717 0.634 0.52668 ## collcat3 -21.28174 16.87625 -1.261 0.20804 ## meals -4.13839 0.15484 -26.726 &lt; 2e-16 *** ## collcat2:meals 0.02815 0.22250 0.127 0.89939 ## collcat3:meals 0.80897 0.25616 3.158 0.00171 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60.53 on 394 degrees of freedom ## Multiple R-squared: 0.8212, Adjusted R-squared: 0.8189 ## F-statistic: 361.9 on 5 and 394 DF, p-value: &lt; 2.2e-16 Test significance of interaction anova(api00.lm1, api00.lm2) ## Analysis of Variance Table ## ## Model 1: api00 ~ collcat + meals ## Model 2: api00 ~ collcat * meals ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 396 1486720 ## 2 394 1443742 2 42978 5.8644 0.003092 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 yhat2.api00 &lt;- predict(api00.lm2, newdata = newdd) pred2.plot &lt;- data.frame(cbind(newdd, yhat2 = yhat2.api00)) ggplot(pred2.plot, aes(meals, yhat2, group = collcat)) + geom_line(aes(colour = collcat, linetype = collcat)) Compare slopes: 3 vs.Â 1 and 2 2 vs 1 Need to do this using contrast matrix. 5.8 Interaction plot in ggplot2 Formatting is pretty heavy but was done this way for someone totalC &lt;- read.csv(&quot;data/total carbon.csv&quot;) str(totalC) ## &#39;data.frame&#39;: 28 obs. of 4 variables: ## $ TC_mgl : num 88.6 96.4 97.6 114 114 ... ## $ Treatment: chr &quot;Control&quot; &quot;NPK&quot; &quot;NPK&quot; &quot;NPK&quot; ... ## $ Day : int 5 5 5 5 5 5 5 15 15 15 ... ## $ Tube : int 1 2 3 4 5 6 7 1 2 3 ... ixn.plot &lt;- ggplot(data = totalC, aes(x = Day, y = TC_mgl, group = Treatment, col = Treatment)) + stat_summary(fun.y = mean, geom = &quot;point&quot;) + stat_summary(fun.y = mean, geom = &quot;line&quot;, lwd = 1) + labs(y = &quot;Total C Concentration (mg/L)&quot;) ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: `fun.y` is deprecated. Use `fun` instead. ixn.plot + theme(panel.background = element_rect( fill = &quot;white&quot;, colour = &quot;black&quot;), legend.text = element_text(size = 12), legend.title = element_text(size = 14), axis.text = element_text(size = 14, face = &quot;bold&quot;, colour = &quot;black&quot;), axis.title = element_text(size = 14, face = &quot;bold&quot;)) 5.9 Plot simple linear regression fitted line with 95 % confidence band Two examples: Jakeâs cycling data and Davidâs water quality data ## Jake&#39;s cycling data cycle &lt;- read.table(&quot;data/cycling.txt&quot;, header = T) summary(cycle) ## vehicle colour pass_distance street ## Min. :1.000 Min. : 1.000 Min. :0.394 Min. :1.000 ## 1st Qu.:1.000 1st Qu.: 1.000 1st Qu.:1.303 1st Qu.:3.000 ## Median :1.000 Median : 3.000 Median :1.529 Median :5.000 ## Mean :1.634 Mean : 3.556 Mean :1.564 Mean :4.396 ## 3rd Qu.:2.000 3rd Qu.: 4.000 3rd Qu.:1.790 3rd Qu.:5.000 ## Max. :7.000 Max. :99.000 Max. :3.787 Max. :6.000 ## Time hour helmet ## Length:2355 Length:2355 Min. :0.0000 ## Class :character Class :character 1st Qu.:0.0000 ## Mode :character Mode :character Median :0.0000 ## Mean :0.4879 ## 3rd Qu.:1.0000 ## Max. :1.0000 ## kerb Bikelane City ## Min. :0.2500 Min. :0.00000 Min. :1.000 ## 1st Qu.:0.2500 1st Qu.:0.00000 1st Qu.:1.000 ## Median :0.5000 Median :0.00000 Median :1.000 ## Mean :0.6702 Mean :0.02123 Mean :1.191 ## 3rd Qu.:1.0000 3rd Qu.:0.00000 3rd Qu.:1.000 ## Max. :1.2500 Max. :1.00000 Max. :2.000 ## date ## Length:2355 ## Class :character ## Mode :character ## ## ## with(cycle, plot(pass_distance ~ kerb, xlim = c(0, 1.5), ylim = c(0, 4))) reg.kerb &lt;- lm(pass_distance ~ kerb, data = cycle) summary(reg.kerb) ## ## Call: ## lm(formula = pass_distance ~ kerb, data = cycle) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.28553 -0.24834 -0.03416 0.20266 2.38263 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.74832 0.01628 107.40 &lt;2e-16 *** ## kerb -0.27516 0.02145 -12.83 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3708 on 2353 degrees of freedom ## Multiple R-squared: 0.06538, Adjusted R-squared: 0.06498 ## F-statistic: 164.6 on 1 and 2353 DF, p-value: &lt; 2.2e-16 range(cycle$kerb) ## [1] 0.25 1.25 kerbnew &lt;- data.frame(kerb = seq(0.25, 1.25, by = 0.05)) ypred &lt;- predict(reg.kerb, newdata = kerbnew, interval = &quot;confidence&quot;) str(ypred) ## num [1:21, 1:3] 1.68 1.67 1.65 1.64 1.62 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:21] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## ..$ : chr [1:3] &quot;fit&quot; &quot;lwr&quot; &quot;upr&quot; ypred[1:8, ] ## fit lwr upr ## 1 1.679526 1.656358 1.702695 ## 2 1.665768 1.644161 1.687376 ## 3 1.652010 1.631865 1.672155 ## 4 1.638252 1.619448 1.657057 ## 5 1.624494 1.606881 1.642108 ## 6 1.610736 1.594131 1.627341 ## 7 1.596978 1.581166 1.612791 ## 8 1.583220 1.567949 1.598491 range(ypred[ , 1]) ## [1] 1.404365 1.679526 ci &lt;- data.frame(lower = ypred[ , &quot;lwr&quot;], upper = ypred[ , &quot;upr&quot;]) with(cycle, plot(pass_distance ~ kerb, xlim = c(0, 1.5), ylim = c(0, 4))) plot(ypred[ , 1] ~ kerbnew$kerb, type = &quot;l&quot;, xlim = c(0, 1.5), ylim = c(1.4, 1.7)) lines(kerbnew$kerb, ci$lower, lty = 2, col = &quot;blue&quot;) lines(kerbnew$kerb, ci$upper, lty = 2, col = &quot;blue&quot;) Davidâs water quality data waterQual &lt;- read.table(&quot;data/waterQual.txt&quot;, header = T) #save(cycle, waterQual, file = &quot;lm pred confidence band.RData&quot;) summary(waterQual) ## catchment quality ## Min. : 5.0 Min. :29.00 ## 1st Qu.:16.0 1st Qu.:45.25 ## Median :26.0 Median :57.50 ## Mean :25.0 Mean :61.30 ## 3rd Qu.:29.5 3rd Qu.:82.00 ## Max. :57.0 Max. :89.00 str(waterQual) ## &#39;data.frame&#39;: 20 obs. of 2 variables: ## $ catchment: int 29 49 28 8 57 9 31 10 21 26 ... ## $ quality : int 61 85 46 53 55 71 59 41 82 56 ... par(cex.axis = 0.9) plot(waterQual$quality ~ waterQual$catchment, axes = FALSE, xlim = c(0, 60), ylim = c(20, 105), frame = T) axis(1, at = 0:6*10) axis(2, at = 2:10*10) qual.lm1 &lt;- lm(quality ~ catchment, data = waterQual) summary(qual.lm1) ## ## Call: ## lm(formula = quality ~ catchment, data = waterQual) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.078 -14.218 -1.309 17.742 23.240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.7945 8.5246 5.841 1.56e-05 *** ## catchment 0.4602 0.2966 1.552 0.138 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.81 on 18 degrees of freedom ## Multiple R-squared: 0.118, Adjusted R-squared: 0.06898 ## F-statistic: 2.408 on 1 and 18 DF, p-value: 0.1381 plot(waterQual$quality ~ waterQual$catchment, axes = FALSE, xlim = c(0, 60), ylim = c(20, 105), frame = T) axis(1, at = 0:6*10) axis(2, at = 2:10*10) abline(qual.lm1) # Get values for plotting 95 % confidence band ## Make set of X values at which to predict response Xdata &lt;- with(waterQual, seq(from = min(catchment), to = max(catchment), length = 50)) ## Put those new X values into a data frame with column name same as X ## variable in fitted model Xnew &lt;- data.frame(catchment = Xdata) ## Use these X values to get predictions for Y from model ypred &lt;- predict(qual.lm1, newdata = Xnew, interval = &quot;confidence&quot;, type = &quot;response&quot;) str(ypred) ## num [1:50, 1:3] 52.1 52.6 53.1 53.6 54 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:50] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## ..$ : chr [1:3] &quot;fit&quot; &quot;lwr&quot; &quot;upr&quot; ypred[1:10, ] ## fit lwr upr ## 1 52.09562 36.81859 67.37266 ## 2 52.58402 37.84145 67.32659 ## 3 53.07241 38.85364 67.29119 ## 4 53.56081 39.85394 67.26768 ## 5 54.04921 40.84096 67.25745 ## 6 54.53760 41.81316 67.26205 ## 7 55.02600 42.76876 67.28323 ## 8 55.51439 43.70580 67.32298 ## 9 56.00279 44.62209 67.38348 ## 10 56.49118 45.51520 67.46716 ## For convenience, put Y alues for confidence band in a data frame ci &lt;- data.frame(lower = ypred[ , &quot;lwr&quot;], upper = ypred[ , &quot;upr&quot;]) ## Put the plot together par(cex.axis = 0.9) plot(waterQual$quality ~ waterQual$catchment, axes = FALSE, xlim = c(0, 60), ylim = c(20, 105), frame = T) axis(1, at = 0:6*10) axis(2, at = 2:10*10) lines(Xnew$catchment, ypred[ , &quot;fit&quot;]) lines(Xnew$catchment, ci$lower, lty = 2, col = &quot;purple&quot;) lines(Xnew$catchment, ci$upper, lty = 2, col = &quot;purple&quot;) "],["linear-mixed-models.html", "6 Linear mixed models 6.1 Finding variance components from lme models", " 6 Linear mixed models 6.1 Finding variance components from lme models estu &lt;- read.csv(&quot;data/Estuaries.csv&quot;) str(estu) ## &#39;data.frame&#39;: 54 obs. of 7 variables: ## $ X : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Modification : chr &quot;Modified&quot; &quot;Modified&quot; &quot;Modified&quot; &quot;Modified&quot; ... ## $ Estuary : chr &quot;JAK&quot; &quot;JAK&quot; &quot;JAK&quot; &quot;JAK&quot; ... ## $ Site : int 1 1 2 2 3 3 4 4 1 1 ... ## $ Hydroid : int 0 0 0 0 1 1 0 0 7 5 ... ## $ Total : int 44 42 32 44 42 48 45 34 29 51 ... ## $ Schizoporella.errata: int 15 8 9 14 6 12 28 1 0 0 ... estu.lme1 &lt;- lme(Total ~ Modification, random = ~ 1 | Estuary, data = estu, correlation = corCompSymm(form = ~ 1 | Estuary)) summary(estu.lme1) ## Linear mixed-effects model fit by REML ## Data: estu ## AIC BIC logLik ## 404.6881 414.4444 -197.3441 ## ## Random effects: ## Formula: ~1 | Estuary ## (Intercept) Residual ## StdDev: 7.424348 9.277184 ## ## Correlation Structure: Compound symmetry ## Formula: ~1 | Estuary ## Parameter estimate(s): ## Rho ## 0 ## Fixed effects: Total ~ Modification ## Value Std.Error DF t-value p-value ## (Intercept) 40.97295 4.726969 47 8.667912 0.0000 ## ModificationPristine -14.47295 6.230091 5 -2.323072 0.0678 ## Correlation: ## (Intr) ## ModificationPristine -0.759 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.3859461 -0.7141963 0.2765803 0.5239747 2.0455630 ## ## Number of Observations: 54 ## Number of Groups: 7 VarCorr(estu.lme1) ## Estuary = pdLogChol(1) ## Variance StdDev ## (Intercept) 55.12094 7.424348 ## Residual 86.06614 9.277184 "],["experimental-design.html", "7 Experimental design 7.1 Sample size by simulation", " 7 Experimental design 7.1 Sample size by simulation From: Study Design, 2hr - Gordana Using pilot data, find values of linear model parameters. In this experiment, I would like to see if there is an effect of Petal length on Sepal length. I have pilot data with 15 observations. data(iris) str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... table(iris$Species) ## ## setosa versicolor virginica ## 50 50 50 Get some pilot data (or get different data from Gordanaâs) # rand_setosa &lt;- sample(1:50, size = 15, replace = F) # pilot &lt;- iris[rand_setosa, ] ## Gordana&#39;s data pilot &lt;- iris[1:15, c(1, 3)] pilot_mod &lt;- lm(Sepal.Length ~ Petal.Length, data = pilot) summary(pilot_mod) ## ## Call: ## lm(formula = Sepal.Length ~ Petal.Length, data = pilot) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.50175 -0.25965 -0.05965 0.14825 1.01404 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.0912 1.0441 3.918 0.00176 ** ## Petal.Length 0.5789 0.7316 0.791 0.44296 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4034 on 13 degrees of freedom ## Multiple R-squared: 0.04595, Adjusted R-squared: -0.02743 ## F-statistic: 0.6262 on 1 and 13 DF, p-value: 0.443 beta_0 &lt;- coef(pilot_mod)[1] # estimated intercept (from pilot) sd &lt;- summary(pilot_mod)$sigma # estimated variability my_range &lt;- range(pilot$Petal.Length) # range of petal length nsim &lt;- 500 # number of simulated datasets Then specify a meaningful effect size (here itâs the slope of Petal length) # ecologically meaningful slope (same scale as data) beta_1 &lt;- 1 N &lt;- 20 # desired sample size sim_dat = data.frame(Sepal.Length = NA, Petal.Length = seq(my_range[1], my_range[2], length = N) ) pval = rep(NA, nsim) for (i in 1:nsim) { mean_y &lt;- beta_0 + beta_1 * sim_dat$Petal.Length sim_dat$Sepal.Length &lt;- rnorm(N, mean = mean_y, sd = sd) m &lt;- lm(Sepal.Length ~ Petal.Length, data = sim_dat) pval[i] &lt;- coef(summary(m))[&quot;Petal.Length&quot;, &quot;Pr(&gt;|t|)&quot;] } # cycle through all N values sum(pval &lt; 0.05) / nsim ## [1] 0.482 "],["count-data.html", "8 Count data 8.1 Count data: tests of independence and homogeneity 8.2 Contingency tables - fixed row and columns totals", " 8 Count data 8.1 Count data: tests of independence and homogeneity We want to compare different groups of categorical data, where the values in each category are counts. 8.1.1 Independence Suppose we want to test the hypothesis that two particular variables are operating independently in some population. To do this, we can take a single sample from the population and classify the individuals in the sample according to the two categorical variables. For example, we have a single sample of 556 pea plants and classify them according to shape (round or wrinkled) and colour (green or yellow). (This example is from Ekstrom, CT &amp; Sorensen, H. 2015. Introduction to Statistical Data Analysis for the Life Sciences. Boca Raton: Chapman &amp; Hall/CRC Press, 2nd ed., p.Â 339.) The specific hypothesis weâre testing here is that pea colour and shape are independent (i.e.Â that is the null hypothesis). The data can be summarised by a 2 \\(\\times\\) 2 table (Round &amp; Wrinkled \\(\\times\\) Green &amp; Yellow): .table { width: 40%; } Shape / Colour Green Yellow Total Round 108 315 423 Wrinkled 32 101 133 Total 140 416 556 peas &lt;- matrix(c(315, 101, 108, 32), nrow = 2) rownames(peas) &lt;- c(&quot;Round&quot;, &quot;Wrinkled&quot;) colnames(peas) &lt;- c(&quot;Yellow&quot;, &quot;Green&quot;) peas peas.chisq &lt;- chisq.test(peas, correct = F) peas.chisq chisqObsPeas &lt;- peas.chisq$statistic pvalPeas &lt;- peas.chisq$p.value The test used is a \\(\\chi^2\\) squared test, with test statistic calculated by \\[ X^2 = \\sum \\frac{(\\mathrm{Observed} - \\mathrm{Expected})^2}{\\mathrm{Expected}} \\; , \\] where the test statistic is calculated over all four cells. Here, \\(X^2 =\\) 0.1163, with 1 degree of freedom, giving \\(P =\\) 0.733. With such a large \\(P\\) value, we fail to reject the null hypothesis and conclude that pea colour and shape are independent. 8.1.2 Homogeneity In another context, suppose we want to test the hypothesis that the survival rates of mice treated with two different sources of Staphylococcus aureus are the same. To do this, we run an experiment where one group of mice is injected with a standard bacterial inoculum and the other is injected with a bacterial inoculum to which penicillin has been added. Each of our experimental animals is randomly allocated to one of the groups. (This example is from Steel, RG et al.Â 1997. Principles and Procedures of Statistics: A Biometrical Approach. Boston: McGraw Hill, 3rd ed., p.Â 511.) The specific (null) hypothesis to be tested is that the two treatments result in the same survival rate. This hypothesis is one of homogeneity, as weâre comparing two sub-populations, one treated with a standard inoculum and another with a penicillin-enhanced inoculum. Treatment / Status Alive Dead Total Standard 8 12 20 Penicillin 48 62 110 Total 56 74 130 mice &lt;- matrix(c(8, 48, 12, 62), nrow = 2) rownames(mice) &lt;- c(&quot;Standard&quot;, &quot;Penicillin&quot;) colnames(mice) &lt;- c(&quot;Alive&quot;, &quot;Dead&quot;) ## Pearson&#39;s chi squared test mice.chisq &lt;- chisq.test(mice, correct = F) chisqObsMice &lt;- mice.chisq$statistic pvalMice &lt;- mice.chisq$p.value Here, the test statistic is calculated in the same way as for the test of independence and gives \\(X^2\\) = 0.0913, with 1 degree of freedom, giving \\(P =\\) 0.763. With such a large \\(P\\) value, we fail to reject the null hypothesis and conclude that there is no evidence to suggest a difference in the survival rates resulting from the two treatments. In the first example, one sample was taken from a population and classified according to two categorical variables to determine if the variables operated independently. In the second example, two random samples were used (i.e.Â two sub-populations) to determine whether the two sub-populations were homogeneous in relation to a particular characteristic (i.e.Â survival rate). In the examples above, assumptions need to be met to ensure the test results are valid. The assumptions are that no expected values are less than 1 and no more than 20 % of expected values (i.e.Â here, none) are less than 5. 8.1.3 Comparing two proportions In the above example of survival rates from S. aureus infection, the hypothesis could have been tested by directly comparing the survival rates of the two sub-populations. If the survival rate of the standard treatment group is denoted by \\(p_1\\) and that of the penicillin treatment group by \\(p_2\\), the null hypothesis is \\(H_0: p_1 = p_2\\). If the two proportions are equal, then the samples can be considered as coming from a single population and so, under the null hypothesis, there is a single, common value of the true survival rate. The test statistic is expressed in terms of this common value of the true proportion. The test statistic is also based on a Normal approximation and is \\[ Z = \\frac{ \\hat{p_1} - \\hat{p_2} }{ \\sqrt{ \\hat{p_{\\mathrm{p}}} (1 - \\hat{p_{\\mathrm{p}}}) (1 / n_{1\\cdot} + 1 / n_{2\\cdot})} } \\; , \\] where \\(\\hat{p}_i, \\; i = 1, 2\\) are the observed survival rates for each group, \\(n_{i\\cdot}, \\; i = 1, 2\\) are the total numbers of observations in each group and \\(\\hat{p_{\\mathrm{p}}}\\) is the pooled survival rate over both groups (as it takes account of the null hypothesis); if \\(n_{ij}, \\; i, j = 1, 2\\), are the observed values in each cell in the table and \\(N\\) is the total number of all observations, the pooled survival rate can be written \\[\\hat{p_{\\mathrm{p}}} = (n_{11} + n_{21}) \\; / \\; N \\, .\\] ## Normal approximation ## This compares two population proportions; here, the ## standard and penicillin proportions mice &lt;- matrix(c(8, 48, 12, 62), nrow = 2) n11 &lt;- mice[1, 1] n12 &lt;- mice[1, 2] n21 &lt;- mice[2, 1] n22 &lt;- mice[2, 2] n1. &lt;- n11 + n12 n2. &lt;- n21 + n22 n.1 &lt;- n11 + n21 n.2 &lt;- n12 + n22 N &lt;- sum(mice) p1.hat &lt;- n11 / n1. p2.hat &lt;- n21 / n2. pbar.hat &lt;- (n11 + n21) / N z.obsMice &lt;- (p1.hat - p2.hat) / sqrt( pbar.hat * (1 - pbar.hat) * (1 / n1. + 1 / n2.) ) ## z.obsMice = -0.3021 pvalMiceZ &lt;- 2 * pnorm(z.obsMice) ## P = 0.7626 #z.obsMice^2 ## z.obs^2 = 0.09126 Observed values are \\(\\hat{p_1} = 8 / 20, \\hat{p_2} = 48 / 110,\\) \\(\\hat{p_{\\mathrm{p}}} = 56 / 130\\), giving \\(z =\\) -0.3021 and, for a two-sided test, \\(P =\\) 0.763. This is the same result as obtained from the \\(\\chi^2\\) test above. Note that the denominator in the formula for the \\(Z\\) statistic above is the standard error of the difference of the two proportions, based on the assumption that there is a common value of the true proportion. To obtain a confidence interval for the difference of the two proportions, we treat the two samples simply as independent and calculate the standard error of the difference of the two proportions as \\(\\sqrt{ \\hat{p_1} (1 - \\hat{p_1}) / n_{1\\cdot} + \\hat{p_2} (1 - \\hat{p_2}) / n_{2\\cdot} }.\\) For a significance test, the Normal approximation can be considered valid when the numbers of successes and failures in each sample are all at least 5 and for a confidence interval when they are all at least 10. (See Moore, DS et al.Â 2014. Introduction to the Practice of Statistics. New York: Freeman and Company, 8th ed., pp.Â 508-522, for a discussion of comparing two proportions.) 8.1.4 Small Counts Small observed values may lead to the assumptions for the \\(\\chi^2\\) test not being met. In that case, an alternative analysis method may be used. As an example, consider an experiment investigating whether a particular fungicide is carcinogenic. Two groups of mice were used, with one group (treatment group) being fed small amounts of the fungicide via their food and the other (control group) receiving food with no additives. Mice were randomly allocated to one of the groups. After 85 weeks, all animals were sacrificed and examined for tumours. The data collected is shown below. (This example is from Ekstrom, CT &amp; Sorensen, H. 2015. Introduction to Statistical Data Analysis for the Life Sciences. Boca Raton: Chapman &amp; Hall/CRC Press, 2nd ed., p.Â 335.) This is a test of homogeneity of the control and treatment sub-populations. The null hypothesis is that the mice from the two groups have the same probability of developing tumours. .table { width: 50%; } Treatment / Tumour Tumour present No tumour Total Control 5 74 79 Treatment 4 12 16 Total 9 86 95 The first approach might be the \\(\\chi^2\\) test of homogeneity. tumours &lt;- matrix(c(5, 4, 74, 12), nrow = 2) rownames(tumours) &lt;- c(&quot;Control&quot;, &quot;Treatment&quot;) colnames(tumours) &lt;- c(&quot;Tumour present&quot;, &quot;No tumour&quot;) tumours tumours.chisq &lt;- chisq.test(tumours, correct = F) tumours.chisq chisqObsTumours &lt;- tumours.chisq$statistic pvalTumours &lt;- tumours.chisq$p.value tumours.exp &lt;- tumours.chisq$expected As some observed counts are small, itâs good to check the expected values. These are shown below in parentheses next to the observed values. Treatment / Tumour Tumour present No tumour Total Control 5 (7.484) 74 (71.516) 79 Treatment 4 (1.516) 12 (14.484) 16 Total 9 86 95 The \\(\\chi^2\\) test here gives a test statistic of \\(X^2 =\\) 5.4083 with \\(P =\\) 0.02. As there is one expected value less than 5, the assumptions for the test are not met and this result cannot be accepted as valid. tumours &lt;- matrix(c(5, 4, 74, 12), nrow = 2) n11 &lt;- tumours[1, 1] n12 &lt;- tumours[1, 2] n21 &lt;- tumours[2, 1] n22 &lt;- tumours[2, 2] n1. &lt;- n11 + n12 n2. &lt;- n21 + n22 n.1 &lt;- n11 + n21 n.2 &lt;- n12 + n22 N &lt;- sum(tumours) p1.hat &lt;- n11 / n1. p2.hat &lt;- n21 / n2. pbar.hat &lt;- (n11 + n21) / N z.obsTumours &lt;- (p1.hat - p2.hat) / sqrt( pbar.hat * (1 - pbar.hat) * (1 / n1. + 1 / n2.) ) ## z.obsTumours = -2.3256 pvalTumoursZ &lt;- 2 * pnorm(z.obsTumours) ## P = 0.0200 Using a Normal approximation to compare the probabilities of tumours being present in the two groups, we find the observed test statistic is \\(z =\\) -2.3256 with \\(P =\\) 0.02. This gives the same result as the \\(\\chi^2\\) test. A better approach, because of the small counts and the failure of the \\(X^2\\) test assumptions (one expected value less than 5) and the Normal approximation assumptions (one count less than 5), is to use Fisherâs Exact Test. ## Fisher&#39;s Exact Test tumours &lt;- matrix(c(5, 4, 74, 12), nrow = 2) pvalTumoursFisher &lt;- fisher.test(tumours)$p.value There is no test statistic for this test and the \\(P\\) value calculated is exact. This test gives \\(P =\\) 0.041 for a two-sided test, indicating some evidence against the null hypothesis. Along with any \\(P\\) value, itâs very useful to give an effect size. All these tests can be used with tables larger than the \\(2 \\times 2\\) tables here; however, there are some practical limits to how large a table Fisherâs Exact Test can be used for. 8.2 Contingency tables - fixed row and columns totals See notes in Bland, Intro. to Medical Statistics, p.Â 200. In brief â¦ In a retrospective study on bronchitis and respiratory problems, medical records were searched and the following data collected. Neither the row nor column total was set in advance. Neither is fixed, as each is the result of the incidence of bronchitis and the prevalence of chronic cough, as determined by the binomial distribution. Bronchitis No bronchitis Total Cough 26 44 70 No cough 247 1002 1249 Total 273 1046 1319 In contrast, in a designed experiment (randomised controlled trial) on the effect of streptomycin, patients were randomised to the streptomycin or control groups (for description, see Bland, p.Â 9). The column totals are fixed by the design of the experiment. The row total are not fixed but are determined by a random variable. Radiological assessment Streptomycin Control Total Improved 13 5 18 Deteriorated or died 2 12 14 Total 15 17 32 "],["means---fitted-and-marginal.html", "9 Means - fitted and marginal 9.1 Load necessary packages 9.2 Data - âgenotypeâ from MASS package 9.3 Model 9.4 Fitted values using âpredictâ 9.5 Manually fitting model using matrix algebra 9.6 Estimated means using model coefficients and various weights 9.7 Estimates using âeffectsâ package 9.8 Transformations and emmeans", " 9 Means - fitted and marginal 9.1 Load necessary packages library(dplyr) library(ggplot2) library(ggpubr) library(cowplot) library(effects) library(emmeans) library(DAAG) library(ggpubr) 9.2 Data - âgenotypeâ from MASS package Use âgenotypeâ data from MASS package as an example (data obtained from ScheffÃ© (1959)). Brief details about the data follow. Data from a foster feeding experiment with rat mothers and litters of four different genotypes: A, B, I and J. Rat litters were separated from their natural mothers at birth and given to foster mothers to rear. The data frame has the following components: Litter genotype of the litter. Mother genotype of the foster mother. Wt litter average weight gain of the litter, in grams at age 28 days (the source states that the within-litter variability is negligible). data(genotype, package = &quot;MASS&quot;) gtype &lt;- genotype freq.cell &lt;- gtype %&gt;% count(Mother, Litter) freq.cell ## Mother Litter n ## 1 A A 5 ## 2 A B 4 ## 3 A I 3 ## 4 A J 4 ## 5 B A 3 ## 6 B B 5 ## 7 B I 3 ## 8 B J 3 ## 9 I A 4 ## 10 I B 4 ## 11 I I 5 ## 12 I J 3 ## 13 J A 5 ## 14 J B 2 ## 15 J I 3 ## 16 J J 5 Table of cell frequencies freq.cell.mat &lt;- matrix(freq.cell$n, nrow = 4) rownames(freq.cell.mat) &lt;- c(&quot;Litter_A&quot;, &quot;Litter_B&quot;, &quot;Litter_I&quot;, &quot;Litter_J&quot;) colnames(freq.cell.mat) &lt;- c(&quot;Mother_A&quot;, &quot;Mother_B&quot;, &quot;Mother_I&quot;, &quot;Mother_J&quot;) freq.cell &lt;- freq.cell.mat rm(freq.cell.mat) freq.cell ## Mother_A Mother_B Mother_I Mother_J ## Litter_A 5 3 4 5 ## Litter_B 4 5 4 2 ## Litter_I 3 3 5 3 ## Litter_J 4 3 3 5 gtype %&gt;% count(Litter) ## Litter n ## 1 A 17 ## 2 B 15 ## 3 I 14 ## 4 J 15 gtype %&gt;% count(Mother) ## Mother n ## 1 A 16 ## 2 B 14 ## 3 I 16 ## 4 J 15 ggplot(gtype, aes(x = Mother, y = Wt, colour = Litter)) + geom_point() ggplot(gtype, aes(x = Litter, y = Wt, colour = Mother)) + geom_point() ggplot(gtype, aes(x = Litter, y = Wt, colour = Litter)) + geom_point() + facet_wrap( ~ Mother, nrow = 2) + theme(legend.position = &quot;none&quot;) summary(gtype) ## Litter Mother Wt ## A:17 A:16 Min. :36.30 ## B:15 B:14 1st Qu.:48.20 ## I:14 I:16 Median :54.00 ## J:15 J:15 Mean :53.97 ## 3rd Qu.:60.30 ## Max. :69.80 gtype %&gt;% group_by(Litter) %&gt;% summarise(mean_litter = mean(Wt), std.dev_litter = sd(Wt)) ## # A tibble: 4 x 3 ## Litter mean_litter std.dev_litter ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 55.1 8.63 ## 2 B 54.7 7.13 ## 3 I 52.9 11.3 ## 4 J 53.0 5.87 gtype %&gt;% group_by(Mother) %&gt;% summarise(mean_mother = mean(Wt), std.dev_mother = sd(Wt)) ## # A tibble: 4 x 3 ## Mother mean_mother std.dev_mother ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 55.4 9.89 ## 2 B 58.7 7.24 ## 3 I 53.4 6.45 ## 4 J 48.7 6.30 9.3 Model Explain animal weight by Litter and Mother. Order: Litter + Mother gtype.lm1 &lt;- lm(Wt ~ Litter + Mother, data = gtype) summary(gtype.lm1) ## ## Call: ## lm(formula = Wt ~ Litter + Mother, data = gtype) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.425 -5.584 2.499 5.416 13.745 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.909 2.478 22.964 &lt;2e-16 *** ## LitterB -2.025 2.795 -0.725 0.4719 ## LitterI -2.654 2.827 -0.939 0.3520 ## LitterJ -2.021 2.757 -0.733 0.4668 ## MotherB 3.516 2.862 1.229 0.2246 ## MotherI -1.832 2.767 -0.662 0.5107 ## MotherJ -6.755 2.810 -2.404 0.0197 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.776 on 54 degrees of freedom ## Multiple R-squared: 0.2037, Adjusted R-squared: 0.1152 ## F-statistic: 2.302 on 6 and 54 DF, p-value: 0.04732 anova(gtype.lm1) ## Analysis of Variance Table ## ## Response: Wt ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Litter 3 60.2 20.052 0.3317 0.802470 ## Mother 3 775.1 258.360 4.2732 0.008861 ** ## Residuals 54 3264.9 60.461 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Order: Mother + Litter gtype.lm2 &lt;- lm(Wt ~ Mother + Litter, data = gtype) summary(gtype.lm2) ## ## Call: ## lm(formula = Wt ~ Mother + Litter, data = gtype) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.425 -5.584 2.499 5.416 13.745 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.909 2.478 22.964 &lt;2e-16 *** ## MotherB 3.516 2.862 1.229 0.2246 ## MotherI -1.832 2.767 -0.662 0.5107 ## MotherJ -6.755 2.810 -2.404 0.0197 * ## LitterB -2.025 2.795 -0.725 0.4719 ## LitterI -2.654 2.827 -0.939 0.3520 ## LitterJ -2.021 2.757 -0.733 0.4668 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.776 on 54 degrees of freedom ## Multiple R-squared: 0.2037, Adjusted R-squared: 0.1152 ## F-statistic: 2.302 on 6 and 54 DF, p-value: 0.04732 Note model coefficients are the same regardless of order of explanatory terms. anova(gtype.lm2) ## Analysis of Variance Table ## ## Response: Wt ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Mother 3 771.6 257.202 4.2540 0.009055 ** ## Litter 3 63.6 21.211 0.3508 0.788698 ## Residuals 54 3264.9 60.461 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 9.4 Fitted values using âpredictâ Make a prediction grid. pred.comb &lt;- expand.grid(Litter = levels(gtype$Litter), Mother = levels(gtype$Mother)) pred.comb ## Litter Mother ## 1 A A ## 2 B A ## 3 I A ## 4 J A ## 5 A B ## 6 B B ## 7 I B ## 8 J B ## 9 A I ## 10 B I ## 11 I I ## 12 J I ## 13 A J ## 14 B J ## 15 I J ## 16 J J 9.4.1 Fitted values (with interval = âconfidenceâ) This gives a 95% confidence interval for predicted mean. For example, for Litter A with Mother A: Note \\(t_{0.975, 54} = 2.004879\\). \\(56.90907 \\pm 2.004879 \\times 2.478191 = (51.9406, 61.8775)\\) ypred.lm1 &lt;- data.frame(pred.comb, predict(gtype.lm1, newdata = pred.comb, se.fit = T, interval = &quot;confidence&quot;)) names(ypred.lm1)[3:5] &lt;- c(&quot;fit&quot;, &quot;lwr&quot;, &quot;upr&quot;) ypred.lm1 ## Litter Mother fit lwr upr se.fit df ## 1 A A 56.90907 51.94060 61.87755 2.478191 54 ## 2 B A 54.88389 49.61652 60.15126 2.627276 54 ## 3 I A 54.25550 48.75199 59.75900 2.745055 54 ## 4 J A 54.88814 49.64332 60.13297 2.616031 54 ## 5 A B 60.42496 54.94929 65.90063 2.731171 54 ## 6 B B 58.39978 53.24788 63.55167 2.569678 54 ## 7 I B 57.77138 52.13866 63.40411 2.809507 54 ## 8 J B 58.40403 52.82074 63.98732 2.784850 54 ## 9 A I 55.07696 49.91311 60.24082 2.575644 54 ## 10 B I 53.05178 47.77668 58.32688 2.631131 54 ## 11 I I 52.42339 47.32272 57.52405 2.544125 54 ## 12 J I 53.05603 47.60151 58.51056 2.720625 54 ## 13 A J 50.15438 45.11565 55.19311 2.513234 54 ## 14 B J 48.12920 42.38172 53.87668 2.866746 54 ## 15 I J 47.50081 41.91048 53.09114 2.788361 54 ## 16 J J 48.13345 43.02595 53.24095 2.547534 54 ## residual.scale ## 1 7.775661 ## 2 7.775661 ## 3 7.775661 ## 4 7.775661 ## 5 7.775661 ## 6 7.775661 ## 7 7.775661 ## 8 7.775661 ## 9 7.775661 ## 10 7.775661 ## 11 7.775661 ## 12 7.775661 ## 13 7.775661 ## 14 7.775661 ## 15 7.775661 ## 16 7.775661 ypred.lm1.mat &lt;- matrix(ypred.lm1$fit, nrow = 4) rownames(ypred.lm1.mat) &lt;- c(&quot;Litter_A&quot;, &quot;Litter_B&quot;, &quot;Litter_I&quot;, &quot;Litter_J&quot;) colnames(ypred.lm1.mat) &lt;- c(&quot;Mother_A&quot;, &quot;Mother_B&quot;, &quot;Mother_I&quot;, &quot;Mother_J&quot;) ypred.lm1.mat ## Mother_A Mother_B Mother_I Mother_J ## Litter_A 56.90907 60.42496 55.07696 50.15438 ## Litter_B 54.88389 58.39978 53.05178 48.12920 ## Litter_I 54.25550 57.77138 52.42339 47.50081 ## Litter_J 54.88814 58.40403 53.05603 48.13345 9.4.2 Fitted values (with interval = âpredictionâ) This gives a 95% confidence interval for a single future observed value. The SE for a single future observation is (must be!) larger than that for the predicted mean. The SE for a given observed value is \\(\\sqrt{[\\mathrm{var(predicted \\; mean)} + \\widehat{\\sigma^2}]}\\) (see Searle, Linear Models, p.Â 91). For example, for Litter A with Mother A: Note \\(t_{0.975, 54} = 2.004879\\). \\(\\mathrm{se} = \\sqrt{2.478191^2 + 7.775661^2} = 8.161025\\) \\(56.90907 \\pm 2.004879 \\times 8.161025 = (40.5472, 73.2709)\\) predict(gtype.lm1, newdata = pred.comb, se.fit = T, interval = &quot;prediction&quot;) ## $fit ## fit lwr upr ## 1 56.90907 40.54720 73.27094 ## 2 54.88389 38.42879 71.33899 ## 3 54.25550 37.72330 70.78770 ## 4 54.88814 38.44025 71.33604 ## 5 60.42496 43.90200 76.94791 ## 6 58.39978 41.98128 74.81828 ## 7 57.77138 41.19572 74.34705 ## 8 58.40403 41.84510 74.96296 ## 9 55.07696 38.65471 71.49922 ## 10 53.05178 36.59421 69.50936 ## 11 52.42339 36.02089 68.82588 ## 12 53.05603 36.54007 69.57199 ## 13 50.15438 33.77104 66.53773 ## 14 48.12920 31.51419 64.74422 ## 15 47.50081 30.93950 64.06211 ## 16 48.13345 31.72883 64.53807 ## ## $se.fit ## 1 2 3 4 5 6 7 ## 2.478191 2.627276 2.745055 2.616031 2.731171 2.569678 2.809507 ## 8 9 10 11 12 13 14 ## 2.784850 2.575644 2.631131 2.544125 2.720625 2.513234 2.866746 ## 15 16 ## 2.788361 2.547534 ## ## $df ## [1] 54 ## ## $residual.scale ## [1] 7.775661 9.5 Manually fitting model using matrix algebra Model coefficients are the same as above - as they should be! gtype.mat &lt;- model.matrix(gtype.lm1) XtX.inv &lt;- ginv( t(gtype.mat) %*% gtype.mat ) bhat &lt;- XtX.inv %*% t(gtype.mat) %*% gtype$Wt bhat ## [,1] ## [1,] 56.909073 ## [2,] -2.025180 ## [3,] -2.653575 ## [4,] -2.020931 ## [5,] 3.515885 ## [6,] -1.832111 ## [7,] -6.754690 bhat.df &lt;- data.frame(param = colnames(gtype.mat), est = bhat) bhat.df ## param est ## 1 (Intercept) 56.909073 ## 2 LitterB -2.025180 ## 3 LitterI -2.653575 ## 4 LitterJ -2.020931 ## 5 MotherB 3.515885 ## 6 MotherI -1.832111 ## 7 MotherJ -6.754690 9.6 Estimated means using model coefficients and various weights First, get the two-way table of means. means.cell &lt;- genotype %&gt;% group_by(Mother, Litter) %&gt;% summarise(wt_mean = mean(Wt)) ## `summarise()` has grouped output by &#39;Mother&#39;. You can override using the `.groups` argument. ## Form a 4 x 4 table means.cell &lt;- matrix(means.cell$wt_mean, nrow = 4) rownames(means.cell) &lt;- c(&quot;Litter_A&quot;, &quot;Litter_B&quot;, &quot;Litter_I&quot;, &quot;Litter_J&quot;) colnames(means.cell) &lt;- c(&quot;Mother_A&quot;, &quot;Mother_B&quot;, &quot;Mother_I&quot;, &quot;Mother_J&quot;) means.cell ## Mother_A Mother_B Mother_I Mother_J ## Litter_A 63.680 52.40000 54.12500 48.96000 ## Litter_B 52.325 60.64000 53.92500 45.90000 ## Litter_I 47.100 64.36667 51.60000 49.43333 ## Litter_J 54.350 56.10000 54.53333 49.06000 9.6.1 Marginal means for Litter apply(means.cell, 1, mean) ## Litter_A Litter_B Litter_I Litter_J ## 54.79125 53.19750 53.12500 53.51083 These are based on the data and are not the estimates returned by any package here with any weighting. Weightings used to calculate marginal means are not the ones used in the above calculation. This is because estimated marginal means are not based directly on the data but rather are derived from a model. 9.6.1.1 Estimates using âemmeansâ package 9.6.1.2 âReference gridâ for emmeans Estimated marginal means are based on a model, not directly on the raw data. They are calculated using a reference grid, which is a grid of all combinations of factors and covariates in the model. The reference grid for the genotype model is shown; the grid includes cell counts (viz.Â the .wgt. column). ref_grid(gtype.lm1) @ grid ## Litter Mother .wgt. ## 1 A A 5 ## 2 B A 4 ## 3 I A 3 ## 4 J A 4 ## 5 A B 3 ## 6 B B 5 ## 7 I B 3 ## 8 J B 3 ## 9 A I 4 ## 10 B I 4 ## 11 I I 5 ## 12 J I 3 ## 13 A J 5 ## 14 B J 2 ## 15 I J 3 ## 16 J J 5 The model can be used to estimate the mean at each point in the reference grid. ypred.ref.grid.mat &lt;- matrix(predict(ref_grid(gtype.lm1)), nrow = 4) rownames(ypred.ref.grid.mat) &lt;- c(&quot;Litter_A&quot;, &quot;Litter_B&quot;, &quot;Litter_I&quot;, &quot;Litter_J&quot;) colnames(ypred.ref.grid.mat) &lt;- c(&quot;Mother_A&quot;, &quot;Mother_B&quot;, &quot;Mother_I&quot;, &quot;Mother_J&quot;) ypred.ref.grid.mat ## Mother_A Mother_B Mother_I Mother_J ## Litter_A 56.90907 60.42496 55.07696 50.15438 ## Litter_B 54.88389 58.39978 53.05178 48.12920 ## Litter_I 54.25550 57.77138 52.42339 47.50081 ## Litter_J 54.88814 58.40403 53.05603 48.13345 Manually calculate means in reference grid from model. Model equation is: \\[ \\hat{y} = 56.909 - 2.025 L_B - 2.654 L_I - 2.021 L_J + 3.516 M_B - 1.832 M_I - 6.755 M_J \\] Mean for Litter A with Mother A = 56.91 Mean for Litter A with Mother I = 56.909 - 1.832 = 55.08 Mean for Litter I with Mother J = 56.909 - 2.654 - 6.755 = 47.50 This shows the means calculated from the reference grid are the model-based means. Default weighting is âequalâ (i.e.Â equally weighted average). gtype.emm.L &lt;- emmeans(gtype.lm1, specs = &quot;Litter&quot;) gtype.emm.L ## Litter emmean SE df lower.CL upper.CL ## A 55.6 1.90 54 51.8 59.4 ## B 53.6 2.03 54 49.5 57.7 ## I 53.0 2.09 54 48.8 57.2 ## J 53.6 2.02 54 49.6 57.7 ## ## Results are averaged over the levels of: Mother ## Confidence level used: 0.95 gtype.emm.L.eq &lt;- emmeans(gtype.lm1, specs = &quot;Litter&quot;, weights = &quot;equal&quot;) gtype.emm.L.eq ## Litter emmean SE df lower.CL upper.CL ## A 55.6 1.90 54 51.8 59.4 ## B 53.6 2.03 54 49.5 57.7 ## I 53.0 2.09 54 48.8 57.2 ## J 53.6 2.02 54 49.6 57.7 ## ## Results are averaged over the levels of: Mother ## Confidence level used: 0.95 apply(ypred.ref.grid.mat, 1, mean) ## Litter_A Litter_B Litter_I Litter_J ## 55.64134 53.61616 52.98777 53.62041 Using weights = âproportionalâ This option weights according to the number of observations in each level of the factor thatâs being averaged over. See below. gtype.emm.L.prop &lt;- emmeans(gtype.lm1, specs = &quot;Litter&quot;, weights = &quot;proportional&quot;) gtype.emm.L.prop ## Litter emmean SE df lower.CL upper.CL ## A 55.6 1.89 54 51.8 59.4 ## B 53.5 2.03 54 49.5 57.6 ## I 52.9 2.09 54 48.7 57.1 ## J 53.6 2.02 54 49.5 57.6 ## ## Results are averaged over the levels of: Mother ## Confidence level used: 0.95 Get frequencies (number of observations) for each Mother. Using these gives the same result as weights = âproportionalâ. num.moth &lt;- apply(freq.cell, 2, sum) num.moth ## Mother_A Mother_B Mother_I Mother_J ## 16 14 16 15 gtype.emm.L.prop2 &lt;- emmeans(gtype.lm1, specs = &quot;Litter&quot;, weights = num.moth) gtype.emm.L.prop2 ## Litter emmean SE df lower.CL upper.CL ## A 55.6 1.89 54 51.8 59.4 ## B 53.5 2.03 54 49.5 57.6 ## I 52.9 2.09 54 48.7 57.1 ## J 53.6 2.02 54 49.5 57.6 ## ## Results are averaged over the levels of: Mother ## Confidence level used: 0.95 Direct calculation behind weights = âproportionalâ. Get the reference grid for the model. This is the 4 x 4 table of model-based means. For Litter A, multiply each cell mean by the corresponding total number of observations for each Mother (i.e.Â num.moth are the weights). Add the result and divide by the total number of observations. t( ypred.ref.grid.mat %*% num.moth / sum(num.moth) ) ## Litter_A Litter_B Litter_I Litter_J ## [1,] 55.57445 53.54927 52.92088 53.55352 9.6.1.3 Using âconfintâ This gives the same output as asking for the estimated marginal means from a given model for a given predictor (i.e.Â same output as âgtype.emm.Lâ itself). confint(gtype.emm.L, side = &quot;two-sided&quot;, level = 0.95) ## Litter emmean SE df lower.CL upper.CL ## A 55.6 1.90 54 51.8 59.4 ## B 53.6 2.03 54 49.5 57.7 ## I 53.0 2.09 54 48.8 57.2 ## J 53.6 2.02 54 49.6 57.7 ## ## Results are averaged over the levels of: Mother ## Confidence level used: 0.95 9.6.2 Marginal means for Mother Default weighting is âequalâ (i.e.Â equally weighted average). gtype.emm.M &lt;- emmeans(gtype.lm1, specs = &quot;Mother&quot;) gtype.emm.M ## Mother emmean SE df lower.CL upper.CL ## A 55.2 1.95 54 51.3 59.1 ## B 58.8 2.09 54 54.6 62.9 ## I 53.4 1.95 54 49.5 57.3 ## J 48.5 2.04 54 44.4 52.6 ## ## Results are averaged over the levels of: Litter ## Confidence level used: 0.95 gtype.emm.M.eq &lt;- emmeans(gtype.lm1, specs = &quot;Mother&quot;, weights = &quot;equal&quot;) gtype.emm.M.eq ## Mother emmean SE df lower.CL upper.CL ## A 55.2 1.95 54 51.3 59.1 ## B 58.8 2.09 54 54.6 62.9 ## I 53.4 1.95 54 49.5 57.3 ## J 48.5 2.04 54 44.4 52.6 ## ## Results are averaged over the levels of: Litter ## Confidence level used: 0.95 apply(ypred.ref.grid.mat, 2, mean) ## Mother_A Mother_B Mother_I Mother_J ## 55.23415 58.75004 53.40204 48.47946 Using weights = âcellsâ This uses cell frequencies and gives arithmetic means. gtype.emm.L.cells &lt;- emmeans(gtype.lm1, specs = &quot;Litter&quot;, weights = &quot;cells&quot;) gtype.emm.L.cells ## Litter emmean SE df lower.CL upper.CL ## A 55.1 1.89 54 51.3 58.9 ## B 54.7 2.01 54 50.6 58.7 ## I 52.9 2.08 54 48.7 57.1 ## J 53.0 2.01 54 48.9 57.0 ## ## Results are averaged over the levels of: Mother ## Confidence level used: 0.95 9.6.3 Using a matrix with equal weights Use weights for each Litter of \\(\\frac{1}{4}\\). See notes on calculating marginal means by Cornell Stats Consult Unit (see file âmarginal means calc CornellStatsConsult.pdfâ). This gives the same result as the default equal weighting based on the model reference grid (i.e.Â these are the same estimates as emmmeans gives using weights = âequalâ, the default weighting). ## Set up matrix with appropriate values for calculation mean.L.bal.wt &lt;- matrix( c(1, 0, 0, 0, 1/4, 1/4, 1/4, 1, 1, 0, 0, 1/4, 1/4, 1/4, 1, 0, 1, 0, 1/4, 1/4, 1/4, 1, 0, 0, 1, 1/4, 1/4, 1/4), nrow = 4, byrow = T) mean.L.bal.wt ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 1 0 0 0 0.25 0.25 0.25 ## [2,] 1 1 0 0 0.25 0.25 0.25 ## [3,] 1 0 1 0 0.25 0.25 0.25 ## [4,] 1 0 0 1 0.25 0.25 0.25 mean.L.bal.wt %*% bhat.df[ , 2] ## [,1] ## [1,] 55.64134 ## [2,] 53.61616 ## [3,] 52.98777 ## [4,] 53.62041 9.6.4 Using a matrix with proportional weights There are 61 data values in all. The number of observations for each mother are shown below. gtype %&gt;% count(Mother) ## Mother n ## 1 A 16 ## 2 B 14 ## 3 I 16 ## 4 J 15 Calculating marginal means for litters using âproportionalâ weighting across Mother. ## Set up matrix with appropriate values for calculation mean.L.prop.wt &lt;- matrix( c(1, 0, 0, 0, 14/61, 16/61, 15/61, 1, 1, 0, 0, 14/61, 16/61, 15/61, 1, 0, 1, 0, 14/61, 16/61, 15/61, 1, 0, 0, 1, 14/61, 16/61, 15/61), nrow = 4, byrow = T) mean.L.prop.wt ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 1 0 0 0 0.2295082 0.2622951 0.2459016 ## [2,] 1 1 0 0 0.2295082 0.2622951 0.2459016 ## [3,] 1 0 1 0 0.2295082 0.2622951 0.2459016 ## [4,] 1 0 0 1 0.2295082 0.2622951 0.2459016 mean.L.prop.wt %*% bhat.df[ , 2] ## [,1] ## [1,] 55.57445 ## [2,] 53.54927 ## [3,] 52.92088 ## [4,] 53.55352 These are the same estimates as emmmeans gives using weights = âproportionalâ. 9.7 Estimates using âeffectsâ package The âeffectsâ package, by default, uses what is âproportionalâ weighting in the emmeans package. gtype.eff.L &lt;- effect(&quot;Litter&quot;, mod = gtype.lm1) gtype.eff.L.df &lt;- as.data.frame(gtype.eff.L) gtype.eff.L.df ## Litter fit se lower upper ## 1 A 55.57445 1.893599 51.77802 59.37089 ## 2 B 53.54927 2.032817 49.47372 57.62483 ## 3 I 52.92088 2.089806 48.73107 57.11069 ## 4 J 53.55352 2.020382 49.50290 57.60414 gtype.eff.all &lt;- allEffects(gtype.lm1) gtype.eff.all ## model: Wt ~ Litter + Mother ## ## Litter effect ## Litter ## A B I J ## 55.57445 53.54927 52.92088 53.55352 ## ## Mother effect ## Mother ## A B I J ## 55.30511 58.82100 53.47300 48.55042 9.8 Transformations and emmeans Data is âriceâ from DAAG package. The rice data frame has 72 rows and 7 columns. The data are from an experiment that compared wild type (wt) and genetically modified rice plants (ANU843), each with three different chemical treatments (F10, NH4Cl, and NH4NO3). data(rice, package = &quot;DAAG&quot;) str(rice) ## &#39;data.frame&#39;: 72 obs. of 7 variables: ## $ PlantNo : num 1 2 3 4 5 6 7 8 9 10 ... ## $ Block : num 1 1 1 1 1 1 2 2 2 2 ... ## $ RootDryMass : num 56 66 40 43 55 66 41 67 40 35 ... ## $ ShootDryMass: num 132 120 108 134 119 125 98 122 114 82 ... ## $ trt : Factor w/ 6 levels &quot;F10&quot;,&quot;NH4Cl&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ fert : Factor w/ 3 levels &quot;F10&quot;,&quot;NH4Cl&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ variety : Factor w/ 2 levels &quot;wt&quot;,&quot;ANU843&quot;: 1 1 1 1 1 1 1 1 1 1 ... rice$PlantNo &lt;- factor(rice$PlantNo) rice$Block &lt;- factor(rice$Block) rice %&gt;% group_by(Block, PlantNo) %&gt;% count() ## # A tibble: 12 x 3 ## # Groups: Block, PlantNo [12] ## Block PlantNo n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 1 1 6 ## 2 1 2 6 ## 3 1 3 6 ## 4 1 4 6 ## 5 1 5 6 ## 6 1 6 6 ## 7 2 7 6 ## 8 2 8 6 ## 9 2 9 6 ## 10 2 10 6 ## 11 2 11 6 ## 12 2 12 6 rice %&gt;% group_by(fert, variety) %&gt;% count() ## # A tibble: 6 x 3 ## # Groups: fert, variety [6] ## fert variety n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 F10 wt 12 ## 2 F10 ANU843 12 ## 3 NH4Cl wt 12 ## 4 NH4Cl ANU843 12 ## 5 NH4NO3 wt 12 ## 6 NH4NO3 ANU843 12 ggplot(rice, aes(x = Block, y = ShootDryMass)) + geom_boxplot() ggplot(rice, aes(x = PlantNo, y = ShootDryMass)) + geom_jitter(width = 0.1) bw1 &lt;- ggplot(rice, aes(x = fert, y = ShootDryMass)) + geom_boxplot() bw2 &lt;- ggplot(rice, aes(x = variety, y = ShootDryMass)) + geom_boxplot() ggarrange(bw1, bw2, nrow = 1) ggplot(rice, aes(x = fert, y = ShootDryMass, fill = variety)) + geom_boxplot() rice.lm1 &lt;- lm(ShootDryMass ~ Block + fert * variety, data = rice) summary(rice.lm1) ## ## Call: ## lm(formula = ShootDryMass ~ Block + fert * variety, data = rice) ## ## Residuals: ## Min 1Q Median 3Q Max ## -64.333 -9.813 -0.292 11.688 48.667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 115.333 5.609 20.562 &lt; 2e-16 *** ## Block2 -14.000 4.240 -3.302 0.00156 ** ## fertNH4Cl -58.083 7.344 -7.909 4.24e-11 *** ## fertNH4NO3 -35.000 7.344 -4.766 1.10e-05 *** ## varietyANU843 -101.000 7.344 -13.753 &lt; 2e-16 *** ## fertNH4Cl:varietyANU843 97.333 10.386 9.372 1.10e-13 *** ## fertNH4NO3:varietyANU843 99.167 10.386 9.548 5.42e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.99 on 65 degrees of freedom ## Multiple R-squared: 0.7736, Adjusted R-squared: 0.7527 ## F-statistic: 37.01 on 6 and 65 DF, p-value: &lt; 2.2e-16 anova(rice.lm1) ## Analysis of Variance Table ## ## Response: ShootDryMass ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Block 1 3528 3528.0 10.902 0.001563 ** ## fert 2 7019 3509.4 10.845 8.625e-05 *** ## variety 1 22685 22684.5 70.100 6.366e-12 *** ## fert:variety 2 38622 19311.2 59.676 1.933e-15 *** ## Residuals 65 21034 323.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 par(mfrow = c(2, 2)) plot(rice.lm1) par(mfrow = c(1, 1)) Log-transform the response to illustrate the point of this exercise! The analysis above clearly isnât satisfactory but the use of emmeans here is the point. rice$logShootDM &lt;- log(rice$ShootDryMass) bw3 &lt;- ggplot(rice, aes(x = fert, y = logShootDM)) + geom_boxplot() bw4 &lt;- ggplot(rice, aes(x = variety, y = logShootDM)) + geom_boxplot() ggarrange(bw3, bw4, nrow = 1) rice.lm2 &lt;- lm(logShootDM ~ Block + fert * variety, data = rice) summary(rice.lm2) ## ## Call: ## lm(formula = logShootDM ~ Block + fert * variety, data = rice) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.79849 -0.19150 0.06138 0.20177 1.10562 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.6764 0.1376 33.991 &lt; 2e-16 *** ## Block2 -0.0709 0.1040 -0.682 0.498 ## fertNH4Cl -0.7823 0.1801 -4.343 5.03e-05 *** ## fertNH4NO3 -0.3878 0.1801 -2.153 0.035 * ## varietyANU843 -2.8779 0.1801 -15.977 &lt; 2e-16 *** ## fertNH4Cl:varietyANU843 2.8093 0.2547 11.028 &lt; 2e-16 *** ## fertNH4NO3:varietyANU843 2.8522 0.2547 11.196 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4412 on 65 degrees of freedom ## Multiple R-squared: 0.8326, Adjusted R-squared: 0.8171 ## F-statistic: 53.87 on 6 and 65 DF, p-value: &lt; 2.2e-16 Get the estimated marginal means of fertiliser for each variety. Note these estimates are on the log scale. rice_emm_f_v &lt;- emmeans(rice.lm2, pairwise ~ fert | variety) summary(rice_emm_f_v) ## $emmeans ## variety = wt: ## fert emmean SE df lower.CL upper.CL ## F10 4.64 0.127 65 4.39 4.90 ## NH4Cl 3.86 0.127 65 3.60 4.11 ## NH4NO3 4.25 0.127 65 4.00 4.51 ## ## variety = ANU843: ## fert emmean SE df lower.CL upper.CL ## F10 1.76 0.127 65 1.51 2.02 ## NH4Cl 3.79 0.127 65 3.54 4.04 ## NH4NO3 4.23 0.127 65 3.97 4.48 ## ## Results are averaged over the levels of: Block ## Confidence level used: 0.95 ## ## $contrasts ## variety = wt: ## contrast estimate SE df t.ratio p.value ## F10 - NH4Cl 0.782 0.18 65 4.343 0.0001 ## F10 - NH4NO3 0.388 0.18 65 2.153 0.0872 ## NH4Cl - NH4NO3 -0.394 0.18 65 -2.190 0.0805 ## ## variety = ANU843: ## contrast estimate SE df t.ratio p.value ## F10 - NH4Cl -2.027 0.18 65 -11.253 &lt;.0001 ## F10 - NH4NO3 -2.464 0.18 65 -13.681 &lt;.0001 ## NH4Cl - NH4NO3 -0.437 0.18 65 -2.428 0.0466 ## ## Results are averaged over the levels of: Block ## P value adjustment: tukey method for comparing a family of 3 estimates Backtransform the estimates to the natural scale. Note this code comes from the built-in help for emmeans. rice_emm_f_v_bt &lt;- emmeans(rice.lm2, pairwise ~ fert | variety, options = list(tran = &quot;log&quot;), type = &quot;response&quot;) summary(rice_emm_f_v_bt) ## $emmeans ## variety = wt: ## fert response SE df lower.CL upper.CL ## F10 103.65 13.202 65 80.37 133.67 ## NH4Cl 47.40 6.038 65 36.76 61.13 ## NH4NO3 70.33 8.958 65 54.53 90.70 ## ## variety = ANU843: ## fert response SE df lower.CL upper.CL ## F10 5.83 0.743 65 4.52 7.52 ## NH4Cl 44.26 5.637 65 34.32 57.08 ## NH4NO3 68.54 8.730 65 53.14 88.39 ## ## Results are averaged over the levels of: Block ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale ## ## $contrasts ## variety = wt: ## contrast ratio SE df null t.ratio p.value ## F10 / NH4Cl 2.1866 0.3939 65 1 4.343 0.0001 ## F10 / NH4NO3 1.4738 0.2655 65 1 2.153 0.0872 ## NH4Cl / NH4NO3 0.6740 0.1214 65 1 -2.190 0.0805 ## ## variety = ANU843: ## contrast ratio SE df null t.ratio p.value ## F10 / NH4Cl 0.1317 0.0237 65 1 -11.253 &lt;.0001 ## F10 / NH4NO3 0.0851 0.0153 65 1 -13.681 &lt;.0001 ## NH4Cl / NH4NO3 0.6457 0.1163 65 1 -2.428 0.0466 ## ## Results are averaged over the levels of: Block ## P value adjustment: tukey method for comparing a family of 3 estimates ## Tests are performed on the log scale Further, to get confidence intervals for the contrasts, do the following. Note that you use the emmeans object, not the summary of that object! confint(rice_emm_f_v_bt$contrasts) ## variety = wt: ## contrast ratio SE df lower.CL upper.CL ## F10 / NH4Cl 2.1866 0.3939 65 1.4195 3.368 ## F10 / NH4NO3 1.4738 0.2655 65 0.9567 2.270 ## NH4Cl / NH4NO3 0.6740 0.1214 65 0.4376 1.038 ## ## variety = ANU843: ## contrast ratio SE df lower.CL upper.CL ## F10 / NH4Cl 0.1317 0.0237 65 0.0855 0.203 ## F10 / NH4NO3 0.0851 0.0153 65 0.0552 0.131 ## NH4Cl / NH4NO3 0.6457 0.1163 65 0.4192 0.995 ## ## Results are averaged over the levels of: Block ## Confidence level used: 0.95 ## Conf-level adjustment: tukey method for comparing a family of 3 estimates ## Intervals are back-transformed from the log scale "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
