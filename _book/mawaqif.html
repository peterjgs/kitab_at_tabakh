<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Mawaqif | Kitab at-Tabakh</title>
  <meta name="description" content="This is a collection of miscellaneous statistics-related recipes and notes." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Mawaqif | Kitab at-Tabakh" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a collection of miscellaneous statistics-related recipes and notes." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Mawaqif | Kitab at-Tabakh" />
  
  <meta name="twitter:description" content="This is a collection of miscellaneous statistics-related recipes and notes." />
  

<meta name="author" content="Peter Geelan-Small" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="ggplot.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Intro</a></li>
<li class="chapter" data-level="1" data-path="mawaqif.html"><a href="mawaqif.html"><i class="fa fa-check"></i><b>1</b> Mawaqif</a>
<ul>
<li class="chapter" data-level="1.1" data-path="mawaqif.html"><a href="mawaqif.html#confounders"><i class="fa fa-check"></i><b>1.1</b> Confounders</a></li>
<li class="chapter" data-level="1.2" data-path="mawaqif.html"><a href="mawaqif.html#exponential-models"><i class="fa fa-check"></i><b>1.2</b> Exponential models</a></li>
<li class="chapter" data-level="1.3" data-path="mawaqif.html"><a href="mawaqif.html#power-function"><i class="fa fa-check"></i><b>1.3</b> Power function</a></li>
<li class="chapter" data-level="1.4" data-path="mawaqif.html"><a href="mawaqif.html#variable-selection"><i class="fa fa-check"></i><b>1.4</b> Variable selection</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="mawaqif.html"><a href="mawaqif.html#davison-2003-statistical-models"><i class="fa fa-check"></i><b>1.4.1</b> Davison 2003, Statistical Models</a></li>
<li class="chapter" data-level="1.4.2" data-path="mawaqif.html"><a href="mawaqif.html#faraway-2014-linear-models-with-r"><i class="fa fa-check"></i><b>1.4.2</b> Faraway 2014, Linear Models with R</a></li>
<li class="chapter" data-level="1.4.3" data-path="mawaqif.html"><a href="mawaqif.html#faraway-2016-extending-the-linear-model-with-r-2nd-ed."><i class="fa fa-check"></i><b>1.4.3</b> Faraway 2016, Extending the Linear Model with R, 2nd ed.</a></li>
<li class="chapter" data-level="1.4.4" data-path="mawaqif.html"><a href="mawaqif.html#weisberg-2014-applied-linear-regression-4th-ed."><i class="fa fa-check"></i><b>1.4.4</b> Weisberg 2014, Applied Linear Regression, 4th ed.</a></li>
<li class="chapter" data-level="1.4.5" data-path="mawaqif.html"><a href="mawaqif.html#harrell-2015-regression-modeling-strategies-2nd-ed."><i class="fa fa-check"></i><b>1.4.5</b> Harrell 2015, Regression Modeling Strategies, 2nd ed.</a></li>
<li class="chapter" data-level="1.4.6" data-path="mawaqif.html"><a href="mawaqif.html#david-wartons-draft-book-chapter-5"><i class="fa fa-check"></i><b>1.4.6</b> David Warton’s draft book, chapter 5</a></li>
<li class="chapter" data-level="1.4.7" data-path="mawaqif.html"><a href="mawaqif.html#james-et-al.-introduction-to-statistical-learning-2017-corrected-printing"><i class="fa fa-check"></i><b>1.4.7</b> James et al. Introduction to Statistical Learning, 2017 (corrected printing)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ggplot.html"><a href="ggplot.html"><i class="fa fa-check"></i><b>2</b> ggplot</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ggplot.html"><a href="ggplot.html#vignettes"><i class="fa fa-check"></i><b>2.1</b> Vignettes</a></li>
<li class="chapter" data-level="2.2" data-path="ggplot.html"><a href="ggplot.html#confidence-bands"><i class="fa fa-check"></i><b>2.2</b> Confidence bands</a></li>
<li class="chapter" data-level="2.3" data-path="ggplot.html"><a href="ggplot.html#histogram"><i class="fa fa-check"></i><b>2.3</b> Histogram</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="ggplot.html"><a href="ggplot.html#histogram-where-data-is-in-one-vector-of-values"><i class="fa fa-check"></i><b>2.3.1</b> Histogram where data is in one vector of values</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ggplot.html"><a href="ggplot.html#density-plot"><i class="fa fa-check"></i><b>2.4</b> Density plot</a></li>
<li class="chapter" data-level="2.5" data-path="ggplot.html"><a href="ggplot.html#plot-structure"><i class="fa fa-check"></i><b>2.5</b> Plot structure</a></li>
<li class="chapter" data-level="2.6" data-path="ggplot.html"><a href="ggplot.html#label-legend"><i class="fa fa-check"></i><b>2.6</b> Label legend</a></li>
<li class="chapter" data-level="2.7" data-path="ggplot.html"><a href="ggplot.html#plot-title"><i class="fa fa-check"></i><b>2.7</b> Plot title</a></li>
<li class="chapter" data-level="2.8" data-path="ggplot.html"><a href="ggplot.html#three-category-colours"><i class="fa fa-check"></i><b>2.8</b> Three-category colours</a></li>
<li class="chapter" data-level="2.9" data-path="ggplot.html"><a href="ggplot.html#set-breaks-for-scale"><i class="fa fa-check"></i><b>2.9</b> Set breaks for scale</a></li>
<li class="chapter" data-level="2.10" data-path="ggplot.html"><a href="ggplot.html#display-table-to-2-d.p."><i class="fa fa-check"></i><b>2.10</b> Display table to 2 d.p.</a></li>
<li class="chapter" data-level="2.11" data-path="ggplot.html"><a href="ggplot.html#redefine-factor-levels-for-an-individual-plot"><i class="fa fa-check"></i><b>2.11</b> Redefine factor levels for an individual plot</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="r-markdown.html"><a href="r-markdown.html"><i class="fa fa-check"></i><b>3</b> R Markdown</a>
<ul>
<li class="chapter" data-level="3.1" data-path="r-markdown.html"><a href="r-markdown.html#ioslides-footnotes"><i class="fa fa-check"></i><b>3.1</b> ioslides: Footnotes</a></li>
<li class="chapter" data-level="3.2" data-path="r-markdown.html"><a href="r-markdown.html#ioslides-speakers-notes"><i class="fa fa-check"></i><b>3.2</b> ioslides: Speaker’s notes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>4</b> Tidyverse</a>
<ul>
<li class="chapter" data-level="4.1" data-path="tidyverse.html"><a href="tidyverse.html#stacking-and-unstacking-data"><i class="fa fa-check"></i><b>4.1</b> Stacking and unstacking data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>5</b> Linear Models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="linear-models.html"><a href="linear-models.html#anova-orthogonal-factors"><i class="fa fa-check"></i><b>5.1</b> ANOVA: Orthogonal factors</a></li>
<li class="chapter" data-level="5.2" data-path="linear-models.html"><a href="linear-models.html#multiple-linear-regression---conditional-and-marginal-effects"><i class="fa fa-check"></i><b>5.2</b> Multiple linear regression - conditional and marginal effects</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="linear-models.html"><a href="linear-models.html#marginal-and-conditional-interpretations"><i class="fa fa-check"></i><b>5.2.1</b> Marginal and Conditional interpretations</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="linear-models.html"><a href="linear-models.html#linear-model-with-subsampling"><i class="fa fa-check"></i><b>5.3</b> Linear model with subsampling</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="linear-models.html"><a href="linear-models.html#load-necessary-r-packages"><i class="fa fa-check"></i><b>5.3.1</b> Load necessary R packages</a></li>
<li class="chapter" data-level="" data-path="linear-models.html"><a href="linear-models.html#example"><i class="fa fa-check"></i>Example</a></li>
<li class="chapter" data-level="5.3.2" data-path="linear-models.html"><a href="linear-models.html#data"><i class="fa fa-check"></i><b>5.3.2</b> Data</a></li>
<li class="chapter" data-level="5.3.3" data-path="linear-models.html"><a href="linear-models.html#summary-statistics-and-plots"><i class="fa fa-check"></i><b>5.3.3</b> Summary statistics and plots</a></li>
<li class="chapter" data-level="5.3.4" data-path="linear-models.html"><a href="linear-models.html#wrong-analysis-fixed-effects-linear-model"><i class="fa fa-check"></i><b>5.3.4</b> Wrong analysis: fixed effects linear model</a></li>
<li class="chapter" data-level="5.3.5" data-path="linear-models.html"><a href="linear-models.html#right-analysis-mixed-effects-linear-model"><i class="fa fa-check"></i><b>5.3.5</b> Right analysis: mixed effects linear model</a></li>
<li class="chapter" data-level="5.3.6" data-path="linear-models.html"><a href="linear-models.html#an-aside---checking-model-assumptions"><i class="fa fa-check"></i><b>5.3.6</b> An aside - checking model assumptions</a></li>
<li class="chapter" data-level="" data-path="linear-models.html"><a href="linear-models.html#equal-variance-assumption"><i class="fa fa-check"></i>Equal variance assumption</a></li>
<li class="chapter" data-level="" data-path="linear-models.html"><a href="linear-models.html#normal-distribution-assumption"><i class="fa fa-check"></i>Normal distribution assumption</a></li>
<li class="chapter" data-level="5.3.7" data-path="linear-models.html"><a href="linear-models.html#fitted-values-and-confidence-intervals"><i class="fa fa-check"></i><b>5.3.7</b> Fitted values and confidence intervals</a></li>
<li class="chapter" data-level="5.3.8" data-path="linear-models.html"><a href="linear-models.html#another-aside---code-the-nested-structure---not-recommended"><i class="fa fa-check"></i><b>5.3.8</b> Another aside - code the nested structure - not recommended</a></li>
<li class="chapter" data-level="5.3.9" data-path="linear-models.html"><a href="linear-models.html#summary"><i class="fa fa-check"></i><b>5.3.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="linear-models.html"><a href="linear-models.html#marginal-means---emmeans-example"><i class="fa fa-check"></i><b>5.4</b> Marginal means - emmeans example</a></li>
<li class="chapter" data-level="5.5" data-path="linear-models.html"><a href="linear-models.html#multiple-regression-example"><i class="fa fa-check"></i><b>5.5</b> Multiple regression example</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="linear-models.html"><a href="linear-models.html#regression-model"><i class="fa fa-check"></i><b>5.5.1</b> Regression model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html"><i class="fa fa-check"></i><b>6</b> Linear mixed models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html#finding-variance-components-from-lme-models"><i class="fa fa-check"></i><b>6.1</b> Finding variance components from lme models</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Kitab at-Tabakh</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mawaqif" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Mawaqif</h1>
<style>
blockquote {
  margin-left: 36px;
  font-size:14px;
}
</style>
<div id="confounders" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Confounders</h2>
<p><a href="https://en.wikipedia.org/wiki/Confounding" class="uri">https://en.wikipedia.org/wiki/Confounding</a></p>
<p><a href="https://www.ucl.ac.uk/child-health/short-courses-events/about-statistical-courses/research-methods-and-statistics/chapter-1-content-0" class="uri">https://www.ucl.ac.uk/child-health/short-courses-events/about-statistical-courses/research-methods-and-statistics/chapter-1-content-0</a></p>
<p><br></p>
</div>
<div id="exponential-models" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Exponential models</h2>
<p><br></p>
<p><span class="math inline">\(y = A e^{bt}\)</span></p>
<p><span class="math inline">\(t = 0, y = 5\)</span></p>
<p><span class="math inline">\(A = 5\)</span></p>
<p><br></p>
<p>Three states:</p>
<p><span class="math inline">\(t = 10, y_1 = 500, y_2 = 100, y_3 = 25\)</span></p>
<p><span class="math inline">\(b = (\log y - \log 5) / t\)</span></p>
<p><span class="math inline">\(b_1 = (\log 500 - \log 5) / 10 = 0.461\)</span></p>
<p><span class="math inline">\(b_2 = (\log 100 - \log 5) / 10 = 0.300\)</span></p>
<p><span class="math inline">\(b_3 = (\log 25 - \log 5) / 10 = 0.161\)</span></p>
<p><br></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="mawaqif.html#cb1-1" aria-hidden="true"></a>tt =<span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">15</span></span>
<span id="cb1-2"><a href="mawaqif.html#cb1-2" aria-hidden="true"></a></span>
<span id="cb1-3"><a href="mawaqif.html#cb1-3" aria-hidden="true"></a>y1 &lt;-<span class="st"> </span>A <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(b_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>tt)</span>
<span id="cb1-4"><a href="mawaqif.html#cb1-4" aria-hidden="true"></a></span>
<span id="cb1-5"><a href="mawaqif.html#cb1-5" aria-hidden="true"></a>y2 &lt;-<span class="st"> </span>A <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(b_<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>tt)</span>
<span id="cb1-6"><a href="mawaqif.html#cb1-6" aria-hidden="true"></a></span>
<span id="cb1-7"><a href="mawaqif.html#cb1-7" aria-hidden="true"></a>y3 &lt;-<span class="st"> </span>A <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(b_<span class="dv">3</span> <span class="op">*</span><span class="st"> </span>tt)</span>
<span id="cb1-8"><a href="mawaqif.html#cb1-8" aria-hidden="true"></a></span>
<span id="cb1-9"><a href="mawaqif.html#cb1-9" aria-hidden="true"></a>dd1 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">time =</span> <span class="kw">rep</span>(tt, <span class="dv">3</span>), <span class="dt">y =</span> <span class="kw">c</span>(y1, y2, y3),</span>
<span id="cb1-10"><a href="mawaqif.html#cb1-10" aria-hidden="true"></a>                  <span class="dt">batch =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">each =</span> <span class="kw">length</span>(tt)))</span>
<span id="cb1-11"><a href="mawaqif.html#cb1-11" aria-hidden="true"></a></span>
<span id="cb1-12"><a href="mawaqif.html#cb1-12" aria-hidden="true"></a>dd1<span class="op">$</span>batch &lt;-<span class="st"> </span><span class="kw">factor</span>(dd1<span class="op">$</span>batch)</span>
<span id="cb1-13"><a href="mawaqif.html#cb1-13" aria-hidden="true"></a></span>
<span id="cb1-14"><a href="mawaqif.html#cb1-14" aria-hidden="true"></a><span class="kw">ggplot</span>(dd1, <span class="kw">aes</span>(<span class="dt">x =</span> time, <span class="dt">y =</span> y, <span class="dt">colour =</span> batch)) <span class="op">+</span></span>
<span id="cb1-15"><a href="mawaqif.html#cb1-15" aria-hidden="true"></a><span class="st">    </span><span class="kw">geom_line</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p><br></p>
<p><span class="math inline">\(y = A e^{bt}\)</span></p>
<p>is the same as</p>
<p><span class="math inline">\(\log y = \log A + bt\)</span></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="mawaqif.html#cb2-1" aria-hidden="true"></a>dd1<span class="op">$</span>log_y &lt;-<span class="st"> </span><span class="kw">log</span>(dd1<span class="op">$</span>y)</span>
<span id="cb2-2"><a href="mawaqif.html#cb2-2" aria-hidden="true"></a></span>
<span id="cb2-3"><a href="mawaqif.html#cb2-3" aria-hidden="true"></a><span class="kw">ggplot</span>(dd1, <span class="kw">aes</span>(<span class="dt">x =</span> time, <span class="dt">y =</span> log_y, <span class="dt">colour =</span> batch)) <span class="op">+</span></span>
<span id="cb2-4"><a href="mawaqif.html#cb2-4" aria-hidden="true"></a><span class="st">    </span><span class="kw">geom_line</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-3-1.png" width="672" />
<br></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="mawaqif.html#cb3-1" aria-hidden="true"></a><span class="kw">ggplot</span>(dd1, <span class="kw">aes</span>(<span class="dt">x =</span> time, <span class="dt">y =</span> y, <span class="dt">colour =</span> batch)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb3-2"><a href="mawaqif.html#cb3-2" aria-hidden="true"></a><span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb3-3"><a href="mawaqif.html#cb3-3" aria-hidden="true"></a><span class="st">    </span><span class="kw">coord_trans</span>(<span class="dt">y =</span> <span class="st">&#39;log&#39;</span>) <span class="op">+</span></span>
<span id="cb3-4"><a href="mawaqif.html#cb3-4" aria-hidden="true"></a><span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">8</span><span class="op">*</span><span class="dv">2</span>)) <span class="op">+</span></span>
<span id="cb3-5"><a href="mawaqif.html#cb3-5" aria-hidden="true"></a><span class="st">    </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">40</span>, <span class="dv">80</span>, <span class="dv">160</span>, <span class="dv">325</span>, </span>
<span id="cb3-6"><a href="mawaqif.html#cb3-6" aria-hidden="true"></a>                                  <span class="dv">650</span>, <span class="dv">1300</span>, <span class="dv">2600</span>, <span class="dv">5200</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-4-1.png" width="672" />
<br></p>
<p>For a one unit increase in time, the change in <span class="math inline">\(y\)</span> is:</p>
<p><span class="math inline">\(\log y_j - \log y_i = b\)</span></p>
<p>So, <span class="math inline">\(y_j = e^b y_i\)</span></p>
<p>For batch 1, <span class="math inline">\(e^b = e^{0.461} = 1.59\)</span>, so for each one unit increase in time, <span class="math inline">\(y\)</span> increases by a factor of 1.59 or by 59%.</p>
<p><br></p>
<p>Values of batch 1 at times from 0 to 15:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="mawaqif.html#cb4-1" aria-hidden="true"></a><span class="kw">round</span>(y1, <span class="dt">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##  [1]    5.000    7.924   12.559   19.905   31.548   50.000   79.245
##  [8]  125.594  199.054  315.479  500.000  792.447 1255.943 1990.536
## [15] 3154.787 5000.000</code></pre>
<p><br></p>
<p>Ratios of <span class="math inline">\(y_{1, k} \, / \, y_{1, k-1}\)</span> for some values are shown below, noting that <span class="math inline">\(e^{b_1} =\)</span> 1.585.</p>
<p>For an increase of two units in time,
<span class="math inline">\(\log y_j - \log y_i = 2b\)</span>, so <span class="math inline">\(y_j = e^{2b} y_i\)</span>.</p>
<p><span class="math inline">\(\;\;e^{2b_1} =\)</span> 2.512</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="mawaqif.html#cb6-1" aria-hidden="true"></a>rat_<span class="dv">21</span> &lt;-<span class="st"> </span><span class="kw">round</span>(y1[<span class="dv">2</span>] <span class="op">/</span><span class="st"> </span>y1[<span class="dv">1</span>], <span class="dt">digits =</span> <span class="dv">3</span>)</span>
<span id="cb6-2"><a href="mawaqif.html#cb6-2" aria-hidden="true"></a></span>
<span id="cb6-3"><a href="mawaqif.html#cb6-3" aria-hidden="true"></a>rat_<span class="dv">32</span> &lt;-<span class="st"> </span><span class="kw">round</span>(y1[<span class="dv">3</span>] <span class="op">/</span><span class="st"> </span>y1[<span class="dv">2</span>], <span class="dt">digits =</span> <span class="dv">3</span>)</span>
<span id="cb6-4"><a href="mawaqif.html#cb6-4" aria-hidden="true"></a></span>
<span id="cb6-5"><a href="mawaqif.html#cb6-5" aria-hidden="true"></a>rat_<span class="dv">10</span>_<span class="dv">9</span> &lt;-<span class="st"> </span><span class="kw">round</span>(y1[<span class="dv">10</span>] <span class="op">/</span><span class="st"> </span>y1[<span class="dv">9</span>], <span class="dt">digits =</span> <span class="dv">3</span>)</span>
<span id="cb6-6"><a href="mawaqif.html#cb6-6" aria-hidden="true"></a></span>
<span id="cb6-7"><a href="mawaqif.html#cb6-7" aria-hidden="true"></a>rat_<span class="dv">15</span>_<span class="dv">13</span> &lt;-<span class="st"> </span><span class="kw">round</span>(y1[<span class="dv">15</span>] <span class="op">/</span><span class="st"> </span>y1[<span class="dv">13</span>], <span class="dt">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="center">Times</th>
<th>Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1 &amp; 2</td>
<td>1.585</td>
</tr>
<tr class="even">
<td align="center">2 &amp; 3</td>
<td>1.585</td>
</tr>
<tr class="odd">
<td align="center">9 &amp; 10</td>
<td>1.585</td>
</tr>
<tr class="even">
<td align="center">13 &amp; 15</td>
<td>2.512</td>
</tr>
</tbody>
</table>
<p><br></p>
<p><strong>Doubling time</strong></p>
<p>When has <span class="math inline">\(y\)</span> reached double its initial value - i.e. when is <span class="math inline">\(y = 10\)</span>? Doubling time is <span class="math inline">\(t_{\mathrm{dbl}}\)</span>.</p>
<p><span class="math inline">\(e^{bt_{\mathrm{dbl}}} = 2\)</span></p>
<p><span class="math inline">\(t_{\mathrm{dbl}} = \log 2 / b\)</span></p>
<p><br></p>
<p>For batch 1: <span class="math inline">\(t_{\mathrm{dbl}} = \log 2 / 0.461 = 1.51\)</span></p>
<p>For batch 2: <span class="math inline">\(t_{\mathrm{dbl}} = \log 2 / 0.300 = 2.31\)</span></p>
<p>For batch 1: <span class="math inline">\(t_{\mathrm{dbl}} = \log 2 / 0.161 = 4.31\)</span></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="mawaqif.html#cb7-1" aria-hidden="true"></a>t10 &lt;-<span class="st"> </span><span class="dv">5</span></span>
<span id="cb7-2"><a href="mawaqif.html#cb7-2" aria-hidden="true"></a></span>
<span id="cb7-3"><a href="mawaqif.html#cb7-3" aria-hidden="true"></a>t12 &lt;-<span class="st"> </span>t10 <span class="op">+</span><span class="st"> </span>t_dbl_<span class="dv">1</span></span>
<span id="cb7-4"><a href="mawaqif.html#cb7-4" aria-hidden="true"></a></span>
<span id="cb7-5"><a href="mawaqif.html#cb7-5" aria-hidden="true"></a>t22 &lt;-<span class="st"> </span>t10 <span class="op">+</span><span class="st"> </span>t_dbl_<span class="dv">2</span></span>
<span id="cb7-6"><a href="mawaqif.html#cb7-6" aria-hidden="true"></a></span>
<span id="cb7-7"><a href="mawaqif.html#cb7-7" aria-hidden="true"></a>t32 &lt;-<span class="st"> </span>t10 <span class="op">+</span><span class="st"> </span>t_dbl_<span class="dv">3</span></span>
<span id="cb7-8"><a href="mawaqif.html#cb7-8" aria-hidden="true"></a></span>
<span id="cb7-9"><a href="mawaqif.html#cb7-9" aria-hidden="true"></a>y11 &lt;-<span class="st"> </span><span class="kw">round</span>(A <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(b_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>t10), <span class="dt">digits =</span> <span class="dv">2</span>)</span>
<span id="cb7-10"><a href="mawaqif.html#cb7-10" aria-hidden="true"></a></span>
<span id="cb7-11"><a href="mawaqif.html#cb7-11" aria-hidden="true"></a>y21 &lt;-<span class="st"> </span><span class="kw">round</span>(A <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(b_<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>t10), <span class="dt">digits =</span> <span class="dv">2</span>)</span>
<span id="cb7-12"><a href="mawaqif.html#cb7-12" aria-hidden="true"></a></span>
<span id="cb7-13"><a href="mawaqif.html#cb7-13" aria-hidden="true"></a>y31 &lt;-<span class="st"> </span><span class="kw">round</span>(A <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(b_<span class="dv">3</span> <span class="op">*</span><span class="st"> </span>t10), <span class="dt">digits =</span> <span class="dv">2</span>)</span>
<span id="cb7-14"><a href="mawaqif.html#cb7-14" aria-hidden="true"></a></span>
<span id="cb7-15"><a href="mawaqif.html#cb7-15" aria-hidden="true"></a>y12 &lt;-<span class="st"> </span><span class="kw">round</span>(A <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(b_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>t12), <span class="dt">digits =</span> <span class="dv">2</span>)</span>
<span id="cb7-16"><a href="mawaqif.html#cb7-16" aria-hidden="true"></a></span>
<span id="cb7-17"><a href="mawaqif.html#cb7-17" aria-hidden="true"></a>y22 &lt;-<span class="st"> </span><span class="kw">round</span>(A <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(b_<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>t22), <span class="dt">digits =</span> <span class="dv">2</span>)</span>
<span id="cb7-18"><a href="mawaqif.html#cb7-18" aria-hidden="true"></a></span>
<span id="cb7-19"><a href="mawaqif.html#cb7-19" aria-hidden="true"></a>y32 &lt;-<span class="st"> </span><span class="kw">round</span>(A <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(b_<span class="dv">3</span> <span class="op">*</span><span class="st"> </span>t32), <span class="dt">digits =</span> <span class="dv">2</span>)</span></code></pre></div>
<p>Table below shows value of <span class="math inline">\(y\)</span> at time = 5 and then at time = (5 + doubling time).</p>
<table>
<thead>
<tr class="header">
<th align="center">Batch</th>
<th>time 1 ( = 5)</th>
<th>time 2 ( = 5 + t_dbl)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td>50</td>
<td>100</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td>22.36</td>
<td>44.72</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td>11.18</td>
<td>22.36</td>
</tr>
</tbody>
</table>
<p><br></p>
</div>
<div id="power-function" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Power function</h2>
<p><span class="math inline">\(y = Cx^b\)</span></p>
<p>This becomes <span class="math inline">\(\log y = \log C + \log(x^b)\)</span> or
<span class="math inline">\(\log y = c + b \log x\)</span>.</p>
<p>Take <span class="math inline">\(C = 2\)</span> and <span class="math inline">\(b_4 = 0.8, b_5 = 0.4, b_6 = 0.2\)</span></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="mawaqif.html#cb8-1" aria-hidden="true"></a>C &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb8-2"><a href="mawaqif.html#cb8-2" aria-hidden="true"></a></span>
<span id="cb8-3"><a href="mawaqif.html#cb8-3" aria-hidden="true"></a>b_<span class="dv">4</span> &lt;-<span class="st"> </span><span class="fl">0.8</span></span>
<span id="cb8-4"><a href="mawaqif.html#cb8-4" aria-hidden="true"></a></span>
<span id="cb8-5"><a href="mawaqif.html#cb8-5" aria-hidden="true"></a>b_<span class="dv">5</span> &lt;-<span class="st"> </span><span class="fl">0.4</span></span>
<span id="cb8-6"><a href="mawaqif.html#cb8-6" aria-hidden="true"></a></span>
<span id="cb8-7"><a href="mawaqif.html#cb8-7" aria-hidden="true"></a>b_<span class="dv">6</span> &lt;-<span class="st"> </span><span class="fl">0.2</span></span>
<span id="cb8-8"><a href="mawaqif.html#cb8-8" aria-hidden="true"></a></span>
<span id="cb8-9"><a href="mawaqif.html#cb8-9" aria-hidden="true"></a>tt =<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.0001</span>, <span class="dv">15</span>, <span class="dt">by =</span> <span class="fl">0.2</span>)</span>
<span id="cb8-10"><a href="mawaqif.html#cb8-10" aria-hidden="true"></a></span>
<span id="cb8-11"><a href="mawaqif.html#cb8-11" aria-hidden="true"></a>y4 &lt;-<span class="st"> </span>C <span class="op">*</span><span class="st"> </span>tt<span class="op">^</span>b_<span class="dv">4</span></span>
<span id="cb8-12"><a href="mawaqif.html#cb8-12" aria-hidden="true"></a></span>
<span id="cb8-13"><a href="mawaqif.html#cb8-13" aria-hidden="true"></a>y5 &lt;-<span class="st"> </span>C <span class="op">*</span><span class="st"> </span>tt<span class="op">^</span>b_<span class="dv">5</span></span>
<span id="cb8-14"><a href="mawaqif.html#cb8-14" aria-hidden="true"></a></span>
<span id="cb8-15"><a href="mawaqif.html#cb8-15" aria-hidden="true"></a>y6 &lt;-<span class="st"> </span>C <span class="op">*</span><span class="st"> </span>tt<span class="op">^</span>b_<span class="dv">6</span></span>
<span id="cb8-16"><a href="mawaqif.html#cb8-16" aria-hidden="true"></a></span>
<span id="cb8-17"><a href="mawaqif.html#cb8-17" aria-hidden="true"></a>dd2 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">time =</span> <span class="kw">rep</span>(tt, <span class="dv">3</span>), <span class="dt">y =</span> <span class="kw">c</span>(y4, y5, y6),</span>
<span id="cb8-18"><a href="mawaqif.html#cb8-18" aria-hidden="true"></a>                  <span class="dt">batch =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">each =</span> <span class="kw">length</span>(tt)))</span>
<span id="cb8-19"><a href="mawaqif.html#cb8-19" aria-hidden="true"></a></span>
<span id="cb8-20"><a href="mawaqif.html#cb8-20" aria-hidden="true"></a>dd2<span class="op">$</span>batch &lt;-<span class="st"> </span><span class="kw">factor</span>(dd2<span class="op">$</span>batch)</span>
<span id="cb8-21"><a href="mawaqif.html#cb8-21" aria-hidden="true"></a></span>
<span id="cb8-22"><a href="mawaqif.html#cb8-22" aria-hidden="true"></a><span class="kw">ggplot</span>(dd2, <span class="kw">aes</span>(<span class="dt">x =</span> time, <span class="dt">y =</span> y, <span class="dt">colour =</span> batch)) <span class="op">+</span></span>
<span id="cb8-23"><a href="mawaqif.html#cb8-23" aria-hidden="true"></a><span class="st">    </span><span class="kw">geom_line</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p><br></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="mawaqif.html#cb9-1" aria-hidden="true"></a>dd2<span class="op">$</span>log_y &lt;-<span class="st"> </span><span class="kw">log</span>(dd2<span class="op">$</span>y)</span>
<span id="cb9-2"><a href="mawaqif.html#cb9-2" aria-hidden="true"></a></span>
<span id="cb9-3"><a href="mawaqif.html#cb9-3" aria-hidden="true"></a>dd2<span class="op">$</span>log_time &lt;-<span class="st"> </span><span class="kw">log</span>(dd2<span class="op">$</span>time)</span>
<span id="cb9-4"><a href="mawaqif.html#cb9-4" aria-hidden="true"></a></span>
<span id="cb9-5"><a href="mawaqif.html#cb9-5" aria-hidden="true"></a><span class="kw">ggplot</span>(dd2, <span class="kw">aes</span>(<span class="dt">x =</span> log_time, <span class="dt">y =</span> log_y, <span class="dt">colour =</span> batch)) <span class="op">+</span></span>
<span id="cb9-6"><a href="mawaqif.html#cb9-6" aria-hidden="true"></a><span class="st">    </span><span class="kw">geom_line</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="mawaqif.html#cb10-1" aria-hidden="true"></a><span class="kw">ggplot</span>(dd2, <span class="kw">aes</span>(<span class="dt">x =</span> time, <span class="dt">y =</span> y, <span class="dt">colour =</span> batch)) <span class="op">+</span></span>
<span id="cb10-2"><a href="mawaqif.html#cb10-2" aria-hidden="true"></a><span class="st">    </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb10-3"><a href="mawaqif.html#cb10-3" aria-hidden="true"></a><span class="st">    </span><span class="kw">coord_trans</span>(<span class="dt">x =</span> <span class="st">&#39;log&#39;</span>) <span class="op">+</span></span>
<span id="cb10-4"><a href="mawaqif.html#cb10-4" aria-hidden="true"></a><span class="st">    </span><span class="kw">coord_trans</span>(<span class="dt">y =</span> <span class="st">&#39;log&#39;</span>) <span class="op">+</span></span>
<span id="cb10-5"><a href="mawaqif.html#cb10-5" aria-hidden="true"></a><span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">8</span><span class="op">*</span><span class="dv">2</span>)) <span class="op">+</span></span>
<span id="cb10-6"><a href="mawaqif.html#cb10-6" aria-hidden="true"></a><span class="st">    </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">6</span><span class="op">*</span><span class="dv">3</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p><br></p>
</div>
<div id="variable-selection" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Variable selection</h2>
<div id="davison-2003-statistical-models" class="section level3" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Davison 2003, Statistical Models</h3>
<p>pp. 400-401</p>
<p>Automatic variable selection methods can fit complicated models to random data.</p>
<p>They are widely used but have no theoretical basis.</p>
<p>The rules used for deciding which variables to include vary and are arbitrary - various cut-offs for t or F values are used, for example; sometimes AIC is used.</p>
<p>These methods can be useful for screening.</p>
<p>Knowledge of the system being studied is essential in model building.</p>
<p>p. 402</p>
<p>Some measure of how well a model fits is a reasonable criterion to use.</p>
<p>Residual SS decreases the more terms are added, so this is not satisfctory.</p>
<p>A useful approach is to penalise a model according to its complexity and balance this against a measure of how well the model fits the data.</p>
<p>AIC is one approach of this type. It provides a balance between the fit of the model and its simplicity.</p>
<p>AICc, the corrected AIC, is best when the number of parameters is comparable to the number of rows of data (i.e. p is comparable to n).</p>
<p>p. 404</p>
<p>There may be a number of models with similar AICcs. If a single model is needed, use expert knowledge to choose it, if possible.</p>
<p>If more than one model is plausible, then accept that as the conclusion and discuss them.</p>
<p>p. 405</p>
<p>If automatic variable selection methods are used, making inferences after this model selection is restricted. The only covariates for which standard confidence intervals are reliable are the ones where the evidence that they be included in the model is very strong.</p>
<p><br></p>
</div>
<div id="faraway-2014-linear-models-with-r" class="section level3" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Faraway 2014, Linear Models with R</h3>
<p>p. 153</p>
<p>Do not use testing-based variable selection methods.</p>
<p>With methods that are based on P values, so much multiple testing results in P values losing their usual meaning so that the overall process lacks validity.</p>
<p>The testing-based procedures are not directly related to the overall objectives, namely, to find the best prediction or explanatory model. Variable selection inflates the statistical significance of variables that are kept in the model. Variables that are dropped are not necessarily unrelated to the response variable; they just do not contribute any extra explanatory value over the ones already in the model. <em>The point of this is a bit unclear.</em> <em>What do other writers say about this?</em></p>
<p>p. 159</p>
<p>Use information-criterion-based methods for model selection (e.g. AIC, BIC, adjusted <span class="math inline">\(R^2\)</span>, Mallow’s <span class="math inline">\(C_p\)</span>)</p>
<p>If a number of models fit about the same as one another, accept it! Look at whether</p>
<ul>
<li><p>they each provide different but plausible explanations of the response</p></li>
<li><p>they give similar predictions</p></li>
<li><p>one or some have better model diagnostic features</p></li>
<li><p>one or some include predictors that are cheaper or easier to measure</p></li>
</ul>
<p>p. 161</p>
<p>Problems in having too many predictors in a regression model include:</p>
<ul>
<li><p>collinearity - this can impede how well a model explains the response</p></li>
<li><p>the quality of predictions from the model can be reduced (Faraway probably means by this that the variance of predictions is increased)</p></li>
</ul>
<p>The apparent contradiction is that more predictors should mean more information! There are methods that help to <em>shrink</em> this extra information into a useful form:</p>
<ul>
<li><p>PCA</p></li>
<li><p>partial least squares (p. 172)</p>
<ul>
<li><p>this constructs orthogonal linear combinations of predictors as new predictors</p></li>
<li><p>these combinations are explicitly constructed to predict Y as well as possible</p></li>
</ul></li>
<li><p>ridge regression (p. 174)</p>
<ul>
<li><p>it produces coefficients which are not very large, which is reasonable if you have a lot of candidate predictors and you think many of them have an effect on the response</p></li>
<li><p>this shrinks coefficients, which are normalised (i.e. centred by their means and scaled by their SDs), towards zero</p></li>
<li><p>it’s a form of penalised regression</p></li>
</ul></li>
<li><p>LASSO</p></li>
</ul>
<p><br></p>
</div>
<div id="faraway-2016-extending-the-linear-model-with-r-2nd-ed." class="section level3" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> Faraway 2016, Extending the Linear Model with R, 2nd ed.</h3>
<p>p. 203</p>
<p>For linear mixed models, information-based criteria can be used. However, there are some problems with LMMs:</p>
<ul>
<li><p>because of dependence in the data, the effective sample size is less than the total number of cases</p></li>
<li><p>it isn’t clear how to count the number of fixed effect parameters and random effect parameters together</p></li>
<li><p>most information-criteria measures are based on the likelihood, which does not behave well when parameters are estimated at the boundary of the parameter space as can happen with variance components</p></li>
</ul>
<p>The AIC can be used when the models to be compared differ only in their fixed effects. If random effects also vary, how to count the number of parameters is a problem.</p>
<p>The BIC, which replaces the penalty of <span class="math inline">\(2 \; \times\)</span> no. of parameters in the AIC with the term, <span class="math inline">\(p\log(n)\)</span>, can also be used. It penalises larger models more heavily than the AIC and favours smaller models than the AIC.</p>
<p><br></p>
</div>
<div id="weisberg-2014-applied-linear-regression-4th-ed." class="section level3" number="1.4.4">
<h3><span class="header-section-number">1.4.4</span> Weisberg 2014, Applied Linear Regression, 4th ed.</h3>
<p>pp. 234-235</p>
<p>Methods for selecting variables in regression modelling depend on the purpose of the analysis:</p>
<ul>
<li><p><em>what is the effect of a focal predictor?</em> - examine the effect of a <em>focal</em> predictor, or a few predictors, on a response - including extra predictors apart from those of interest could help the interpretation or increase the precision of tests and estimates; including too many could decrease the precision</p></li>
<li><p><em>which of a number of potential predictors are active?</em> - determine the predictors that are associated with the response - i.e. distinguish the <em>active</em> predictors from the <em>inactive</em> ones; (estimating coefficients and testing whether coefficients are different from zero are not the main point of “variable discovery”, p. 242)</p></li>
<li><p><em>predict values of a response using the predictors</em> - including too many predictors could produce inaccurate predictions because too much detail in the observed data is incorporated (i.e. overfitting); not including enough predictors can produce inaccurate predictions if important predictors are excluded (i.e. biased predictions, as mentioned by someone below)</p></li>
</ul>
<p><br></p>
<p><em>Looking for active predictors</em></p>
<p>pp. 237ff</p>
<p>One approach is looking at groups of predictors and picking one that maximises some selection criterion.</p>
<ul>
<li>Common criterion is AIC (as well as BIC).</li>
</ul>
<p>p. 239</p>
<ul>
<li><p>Stepwise methods</p>
<ul>
<li><p>these methods are not guaranted to give you the model with the optimal value of the selection criterion but they can be useful (p. 240)</p></li>
<li><p>estimating coefficients and testing whether coefficients are different from zero are not the main point of “variable discovery” (p. 242)</p></li>
<li><p>after selecting a subset of variables, the <span class="math inline">\(t\)</span> values and associated <span class="math inline">\(P\)</span> values for coefficients <em>cannot</em> be trusted (as the <span class="math inline">\(t\)</span> values may not follow a <span class="math inline">\(t\)</span> distribution)</p></li>
</ul></li>
<li><p>methods that select subsets exaggerate significance of, for example, <span class="math inline">\(F\)</span> statistics comparing models and <span class="math inline">\(t\)</span> statistics for coefficients in models</p></li>
</ul>
<p>p. 244</p>
<ul>
<li><p>Regularised methods - LASSO (incl. variant, elastic net)</p>
<ul>
<li>work on the basis that as few predictors as possible are needed to model a response</li>
<li>work well on randomly sampled data and model with no factors or interaction</li>
</ul></li>
</ul>
<p>p. 245</p>
<p><em>Developing a model for prediction</em></p>
<p>Possible approaches</p>
<ul>
<li><p>model averaging (incl. Bayesian model averaging) - v. Hoeting tutorial</p></li>
<li><p>cross-validation</p></li>
<li><p>general question of getting predictions from training data has led to development of <em>machine learning;</em> methods include neural networks, random forests, …</p></li>
</ul>
<p><br></p>
</div>
<div id="harrell-2015-regression-modeling-strategies-2nd-ed." class="section level3" number="1.4.5">
<h3><span class="header-section-number">1.4.5</span> Harrell 2015, Regression Modeling Strategies, 2nd ed.</h3>
<p>pp. 67-68</p>
<p>Stepwise methods of variable selection have no statistical justification.</p>
<p>They result in:</p>
<ul>
<li><span class="math inline">\(R^2\)</span> values that are too high</li>
<li><span class="math inline">\(F\)</span> and <span class="math inline">\(\chi^2\)</span> statistics that are not distributed according to those distributions</li>
<li>P values that are too small</li>
<li>SEs that are too small</li>
<li>model coefficients that are too large</li>
</ul>
<p>p. 69-70</p>
<p>Do not use stepwise variable selection methods; use full-model fits or data reduction methods instead.</p>
<p><em>If you have to use stepwise variable selection:</em></p>
<ul>
<li>first of all, fit the full model and do a global test of no regression</li>
<li>if this global test is not significant, it is not justifiable to pick out individually significant predictors</li>
<li>if the global test is significant, use a stopping rule based on the AIC - i.e. smallest AIC (but there are no stopping rules for data-driven variable selection); if someone insists on using a stopping rule based on P values, use <span class="math inline">\(\alpha = 0.5\)</span></li>
<li>use backwards elimination rather than forwards selection
<ul>
<li>it ensures you see the full model, which is the only one with accurate SEs, residual MS and P values</li>
<li>Lawless and Singhal’s method (implemented with fastbw in rms package) is very efficent</li>
</ul></li>
</ul>
<p>p. 70</p>
<p>Bootstrapping can help you choose between the full and a reduced model.</p>
<p>pp. 71-72</p>
<p>Sometimes variables can be grouped (by subject matter or empirical correlations), tested as a group and kept or removed.</p>
<p>Possibly the most accurately measured variable in a group can be retained (v. p. 79f).</p>
<p>Lasso is a useful penalised estimation technique - it forces some coefficient estimates to be zero and in this way carries out variable selection, while also, however, shrinking the remaining coefficients to take account of the overfitting caused by using a data-based model selection method</p>
<p>Screening each variable singly and using only individually significant variables in a multivariable model can miss variables that become important only after adjusting for other variables.</p>
<p><em>Sample size, overfitting …</em></p>
<p>“Overfitting” happens when the model fits some of the noise and not just the signal; it also happens when you find questionable associations between explanatory variables and the response variable.</p>
<p>A good general recommendation where there is a continuous response variable, for example, is to have no more than one candidate predictor variable per 15 observations (this includes interaction terms, if any). ??? df or terms?</p>
<p>If there are explanatory variables with a narrow range of values, you’ll need a larger sample than the above recommendation.</p>
<p>p. 78 (In section on “shrinkage”)</p>
<p>Penalised methods are very good ways to deal with the “too many variables, too little data” problem (more detail in s. 9.10).</p>
<p>Some useful shrinkage methods are:</p>
<ul>
<li>ridge regression</li>
<li>penalised ML estimation</li>
</ul>
<p>Cross-validation or AIC has to be used to choose the penalty factor.</p>
<p>p. 78 <em>Collinearity</em></p>
<p>When one predictor can be predicted well from the others or expressed as a combination of the others, collinearity is present (i.e. if <span class="math inline">\(\textbf{Xa} = \textbf{0}\)</span>, where <span class="math inline">\(\textbf{X}\)</span> is the incidence matrix and <span class="math inline">\(\textbf{a}\)</span> is a vector of constants).</p>
<p>This can inflate the SEs of the regression coefficients.</p>
<p>It can also make selecting “important” variables unreliable.</p>
<p>When groups of highly correlated variables are identified, they can be tested as a whole set with a multiple d.f. test, rather than using a one d.f. test on a single predictor.</p>
<p>To quantify collinearity, use the variance inflation factor.</p>
<p>p. 79 <em>Data reduction</em></p>
<ul>
<li>Use expert knowledge and previous research to remove unimportant variables from the candidate variables</li>
<li>Remove variables that have distributions that are too narrow</li>
<li>Remove variables that have a large number of missing values</li>
</ul>
<p>pp. 89-90</p>
<p>Recommended approaches</p>
<ul>
<li>Fit a full model without removing non-significant predictors</li>
<li>Use data reduction methods to reduce dimensionality</li>
<li>Use shrinkage (i.e. penalised estimation) to fit a large model and not worry about sample size</li>
</ul>
<p>pp. 94ff <em>Harrell’s summary</em></p>
<p><em>Developing models to find active predictors</em></p>
<ul>
<li><p>Collect as much data as possible; ensure predictor variables have wide distributions</p></li>
<li><p>Frame good hypotheses that specify predictor variables that are relevant, including interactions, if appropriate - do <em>not</em> use the response variable in this process of specifying predictors either graphically, using descriptive statistics or doing hypothesis tests of estimates (i.e. use expert knowledge to “screen” potential candidate predictors)</p></li>
<li><p>Impute values as appropriate - see text for details</p></li>
<li><p>Either use cross-validation or bootstrapping, or hold out some sample data as test data</p></li>
</ul>
<p>The following steps need to be carried out <em>on each bootstrap or cross-validation sample</em>.</p>
<ul>
<li><p>When you can test the model for complexity in a structured way, you might be able to simplify it without using penalisation methods - e.g. test a group of predictors based on one P value (which is hopefully outside range of 0.05 to 0.2)</p></li>
<li><p>Check additivity by testing all prespecified interaction terms; if the test is unequivocal (e.g. P &gt; 0.3), drop all interaction terms.</p></li>
</ul>
<p><em>Developing models for estimating effects</em></p>
<p>These are models for getting point or interval estimates for the response at various combinations of the predictors</p>
<ul>
<li>Parsimonious models are less important here, as including all predictors means there is a greater chance the CI for the effect of interest actually is the size stated; on the other hand, including unimportant predictors increases the variance of predicted values</li>
</ul>
<p><em>Developing models for testing hypotheses</em></p>
<p>Very similar strategy as for models for estimating effects</p>
<ul>
<li><p>Parsimony has little importance as a full model fit, with non-significant variables, gives more accurate P values for variables of interest</p></li>
<li><p>It is important to consider interactions; a well-defined test is whether an interaction should be included or not</p></li>
<li><p>Model validation is not necessary</p></li>
</ul>
<p>p. 118 <em>Simplifying the final model by approximation</em></p>
<p>A model with all prespecified terms usually produces the most accurate predictions on new data</p>
<p>Predictions are conditional on all predictors</p>
<p>If a predictor is not significant, predictions can be obtained by using weights, based on the fraction of values of each level of that predictor - if a factor - in the data, to make the predictions <em>unconditional</em> on that predictor (e.g. average over categories of ethnicity).</p>
<p>If there are several non-significant predictors that need to be averaged over in the above way, the process is unwieldy.</p>
<p>What to do about this is a bit over the top!</p>
<p><br></p>
</div>
<div id="david-wartons-draft-book-chapter-5" class="section level3" number="1.4.6">
<h3><span class="header-section-number">1.4.6</span> David Warton’s draft book, chapter 5</h3>
<p>p. 121</p>
<p>Variable selection or model selection is a type of inference. Inference can take various forms, including:</p>
<ul>
<li>hypothesis testing - seeing whether data are consistent with a particular hypothesis</li>
<li>parameter estimation - finding a plausible range of values (confidence interval) for a parameter of interest</li>
<li>variable (model) selection - finding the set of predictor variables that characterise the true process by which the data are generated (inference by using a sample to draw general conclusions about how well a set of explanatory variables can predict)</li>
</ul>
<p>pp. 118-119</p>
<p>Purpose of model selection:</p>
<ul>
<li>maximise predictive capability - i.e. find the model that would give best predictions of the response given new data</li>
</ul>
<p>You need to choose the “right” number of predictors - i.e. the “right” level of model complexity.</p>
<p>A model that is too small gives biased predictions.</p>
<p>A model that is too big results in predictions with increased variance. This is because the model fits noise as well as signal.</p>
<p>Bias-variance trade-off is inevitable. How find the right balance? It depends on:</p>
<ul>
<li>how much data you’ve got</li>
<li>how relatively complex the models being compared are</li>
<li>how strong the signal is in each model</li>
</ul>
<p>R-squared values cannot take account of how complex models are.</p>
<p>Using P values to remove non-significant predictors is not appropriate as there is no real a priori hypothesis being tested.</p>
<p>p. 121 (again)</p>
<p>Comparing a lot of models is of doubtful value as using the data alone to produce the best model is unreliable.</p>
<p>It’s important to simplify potential models - use expert knowledge to remove unnecessary predictors.</p>
<p><em>Comparing predictive models</em></p>
<p>The easiest way to compare models is to see how well they predict from new data - this is “validation”. Get new data or split data into a training set and a test set - the split needs to be random, the two sets must be independent.</p>
<ul>
<li><p>(<span class="math inline">\(K\)</span>-fold) cross-validation can be used here (p. 125) - split the data into <span class="math inline">\(K\)</span> groups and use each group once as the test data, fitting <span class="math inline">\(K\)</span> models in all; in each modelling run, hold out the particular test group, fit the model to the remaining <span class="math inline">\(K-1\)</span> groups combined and then compare the model predictions to the test observations; pool measures of predictive performance across all the runs.</p></li>
<li><p>How do you choose the size of the training sample? Maybe along the lines of Shao’s method - as sample size increases, use a <em>larger number</em> of observations but a <em>smaller proportion</em> of the total sample size. Some suggestions:</p>
<ul>
<li>leave-one-out (i.e. <span class="math inline">\(N\)</span>-fold c.v.) if n &lt; 20</li>
<li>10-fold c.v. for <span class="math inline">\(20 &lt; n &lt; 100\)</span></li>
<li>5-fold c.v. for <span class="math inline">\(n &gt; 100\)</span></li>
</ul></li>
<li><p>How do you measure predictive performance?</p>
<ul>
<li>For linear regression (assuming constant variance) use mean squared error</li>
<li>Use the whole data set (no splits) and penalise larger models by means of an <em>information criterion</em> - e.g. AIC, BIC (p. 127)</li>
<li>what about the GIC ???</li>
<li>AIC overfits and is good if you want a prediction model; BIC does not overfit and is good for variable selection</li>
</ul></li>
<li><p>Information criteria - advantages and disadvantages</p>
<ul>
<li>no random split in data so same answer and simpler to interpret</li>
<li>less intuitive than cross-validation as they judge predictive capability on new data indirectly (as there is no new data!)</li>
<li>they rely on the model being close to the correct model; cross-validation requires only that the training and test data are independent</li>
</ul></li>
</ul>
<p>p. 130 <em>Subset selection</em></p>
<p>No clear winner among forwards, backwards or all subsets methods</p>
<ul>
<li>All subsets
<ul>
<li>more comprehensive but because so many models may be involved, there’s no guarantee it will yield best model</li>
<li>if there are lots of explanatory variables, it’s not practical</li>
</ul></li>
<li>Backwards elimination
<ul>
<li>not a good idea if there are a large number of explanatory variables, as full model could be quite unstable with some parameters that are not well estimated</li>
</ul></li>
<li>Forwards selection
<ul>
<li>with lots of explanatory variables, this might be a better place to start</li>
<li>not a good method if the optimal model ends up with a lot of terms, as there are lots of steps in getting to that model where things can go wrong</li>
</ul></li>
<li>Multi-collinearity</li>
</ul>
<p>This can undermine how stepwise methods work as the chance of a particular term being added to the model drops substantially if it’s correlated with a term already in the model</p>
<p>p. 134 <em>Penalised estimation</em></p>
<p>A more modern and good way to do subset selection - e.g. LASSO</p>
<p>Parameter estimates are pushed towards zero - this introduces bias but reduces variance, so there’s a bias-variance trade-off here</p>
<p>Penalised estimation is good when:</p>
<ul>
<li>you want to predict the response, as this method reduces variance of predictions</li>
<li>you have lots of terms in your model or not a large sample relative to the number of terms</li>
</ul>
<p>LASSO does variable selection by forcing some parameter estimates to be zero - if the parameter estimate for a term is zero, the terms drops out of the model</p>
<p>LASSO - advantages and disadvantages</p>
<ul>
<li>it does model selection as part of its routine and simplifies this model selection to estimating a single nuisance parameter that is part of the penalty term</li>
<li>it produces good predictions by reducing variance</li>
<li>parameter estimates are, unfortunately, biased</li>
<li>standard errors are not easily available</li>
</ul>
<p><br></p>
</div>
<div id="james-et-al.-introduction-to-statistical-learning-2017-corrected-printing" class="section level3" number="1.4.7">
<h3><span class="header-section-number">1.4.7</span> James et al. Introduction to Statistical Learning, 2017 (corrected printing)</h3>
<p>p. 30 <em>Assessing model accuracy</em></p>
<p>For linear regression models, a common way of measuring how well the predictions a model makes match the observed data is to use the <em>residual mean square</em> or <em>mean squared error.</em></p>
<p>This says how well the model works on the training data used to fit the model (i.e. the <em>training data</em>).</p>
<p>What’s more interesting and important is how well the model would perform on new data (i.e. <em>test data</em>).</p>
<p>For example, suppose we have clinical measurements on a group of patients (e.g. age, BP, gender, height, weight) and whether each patient has diabetes. We can develop a model using this data to predict risk of diabetes. We are particularly interested whether this model accurately predicts the risk of diabetes for <em>future patients.</em></p>
<p>How can you minimise the test MSE?</p>
<p>If test data is available, use this test data set and pick the method that gives smallest MSE.</p>
<p>If no test data is available, you could use the method that gives the smallest MSE for the training data …. But no! Lots of modelling methods specifically minimise the MSE in the fitting process; these methods can give quite a small MSE for the training set but give a much larger MSE on test data.</p>
<p>p. 32 - direct quote</p>
<blockquote>
<p>… as the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function <span class="math inline">\(f\)</span>.</p>
</blockquote>
<p>p. 34</p>
<p>Note explanation of <em>bias</em> and <em>variance</em> in context of modelling (or statistical learning).</p>
<p>Variance is the amount by which estimated response values would change using different training data. See example in test.</p>
<p>Bias is the error caused by approximating a possibly quite complex real-life system by a much simpler model.</p>
<p>As the flexibility of a model increases, the variance increases and the bias decreases.</p>
<p>p. 75</p>
<p>Important questions when doing multiple linear regression:</p>
<ul>
<li>is at least one predictor useful in predicting response?</li>
<li>do all predictors, or only some of them, help to explain the response?</li>
<li>how well does the model fit the data?</li>
<li>given specific values for predictors, what is the predicted response value and how accurate is it?</li>
</ul>
<p>pp. 79 and 208</p>
<p>If <span class="math inline">\(p &gt; n\)</span>, backward elimination cannot be used, as you cannot fit the full model; forward selection can always be used but if <span class="math inline">\(p &gt; n\)</span>, you cannot get a unique solution and can only fit submodels.</p>
<p>p. 92</p>
<p>Brief overview of potential problems with regression model:</p>
<ul>
<li>non-linearity of relationships between response and predictor</li>
<li>correlation of errors (non-independence)</li>
<li>non-constant variance of errors</li>
<li>outliers</li>
<li>points with high leverage</li>
<li>collinearity - note very interesting plots showing effect of collinearity on estimated coefficients - fig. 3.15</li>
</ul>
<p>Chapter 6 Linear model selection</p>
<p>pp. 205-210</p>
<p>All subsets, forward selection, backward elimination and hybrid approaches are outlined.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ggplot.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": null
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
